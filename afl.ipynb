{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d_XrB849SNBz",
   "metadata": {
    "id": "d_XrB849SNBz",
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#@title Parameters\n",
    "\n",
    "# @markdown <h3>Main Parameters</h3>\n",
    "DATASET_NAME = \"MPQA-I\" #@param [\"MPQA-T\", \"MPQA-P\", \"MPQA-I\", \"AGNEWS\", \"AMZN-EN\"]\n",
    "AFL_APPROACH = \"FINE-TUNING\" #@param [\"FINE-TUNING\", \"IN-CONTEXT\"]\n",
    "BASE_MODEL = \"FLAN-T5\" #@param [\"BART\", \"FLAN-T5\", \"Uncharted\"]\n",
    "# @markdown &emsp; ↳ Model name and learning rate will be automatically configured if anything other than uncharted is selected. If uncharted is selected, the model name and learning rate need to be configured manually.\n",
    "AFL_METHOD = \"Rep(En)-ClUn(En)\" #@param [\"Random\", \"Rep(En)\", \"Rep(En)-Un\", \"Rep(En)-Rep(Sc)\", \"Rep(En)-Rep(En)\", \"Rep(En)-UnRep\", \"Rep(En)-ClUn(Sc)\", \"Rep(En)-ClUn(En)\", \"Uncharted\"]\n",
    "# @markdown &emsp; ↳ Embedding and sampling methods will be automatically configured if anything other than uncharted is selected. If uncharted is selected, the embedding and sampling methods need to be configured manually.\n",
    "SAMPLING_ITERATIONS = 10 #@param {type:'slider', min:1, max:32, step:1}\n",
    "NUMBER_OF_SAMPLES_PER_ITERATION = 10 #@param {type:'slider', min:3, max:100, step:1}\n",
    "# @markdown &emsp; ↳ M in paper; K = SAMPLING_ITERATIONS * M\n",
    "\n",
    "# @markdown <br/> <h3> Other Parameters </h3>\n",
    "RUNTIME_TYPE = 'CONDA' #@param [\"COLAB\", \"CONDA\"]\n",
    "EXPERIMENT_NAME = 'test'\n",
    "MAX_NEW_TOKENS = 5 #@param {type:'slider', min:1, max:20, step:1}\n",
    "INITIAL_LEARNING_RATE = 1e-4 #@param {\"type\": \"number\"}\n",
    "LEARNING_RATE_DECAY = 0.98 #@param {type:\"slider\", min:0.5, max:1.0, step:0.05}\n",
    "LEARNING_RATE = INITIAL_LEARNING_RATE\n",
    "\n",
    "FP16 = False\n",
    "LOCAL_RANK = -1\n",
    "FP16_OPT_LEVEL = 'O1'\n",
    "FP16_FULL_EVAL = False\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 10 #@param {type: \"integer\"}\n",
    "PER_DEVICE_VAL_BATCH_SIZE = 256 #@param {type: \"integer\"}\n",
    "PER_DEVICE_VAL_BATCH_SIZE_EMBEDDING = 256#@param {type: \"integer\"}\n",
    "LOGGING_STRATEGY = 'epoch'\n",
    "EVAL_STRATEGY = 'epoch'\n",
    "SAVE_STRATEGY = 'epoch'\n",
    "LOAD_BEST_MODEL_AT_END = True\n",
    "save_total_limit = 1\n",
    "METRIC_FOR_BEST_MODEL = 'eval_main_metric'\n",
    "NUM_TRAIN_EPOCHS = 100\n",
    "EARLY_STOPPING = 20 # Set 0 to disable\n",
    "SEED = 42\n",
    "MODEL_NAME = \"facebook/bart-base\" # @param [\"facebook/bart-base\",\"t5-small\",\"t5-base\",\"t5-large\",\"google/flan-t5-small\",\"google/flan-t5-base\",\"google/flan-t5-large\"] {\"allow-input\":true}\n",
    "EMBEDDING_METHOD_FIRST_ITERATION = \"FLAN-T5-ENCODER\" #@param [\"BART-ENCODER\", \"BART-SCORES\", \"BART-SCORES+ENCODER\", \"T5-ENCODER\", \"FLAN-T5-ENCODER\", \"FLAN-T5-SCORES\", \"FLAN-T5-SCORE+ENCODER\"]\n",
    "EMBEDDING_METHOD_SECOND_ITERATION_PLUS = \"SAME_AS_BEFORE\" #@param [\"SAME_AS_BEFORE\", \"SCORES\", \"SCORES+FLAN-T5\", \"BART\", \"T5\", \"FLAN-T5\"]\n",
    "EMBEDDING_PREFIX = None\n",
    "SAMPLING_METHOD_FIRST_ITERATION = \"KMEANS_REPRESENTATIVE\" #@param [\"BALANCED\", \"RANDOM\", \"KMEANS_RANDOM\", \"KMEANS_REPRESENTATIVE\"]\n",
    "SAMPLING_METHOD_SECOND_ITERATION_PLUS = \"SAME_AS_BEFORE\" #@param [\"SAME_AS_BEFORE\", \"ENTROPY_THEN_KMEANS_REPRESENTATIVE\", \"KMEANS_REPRESENTATIVE_THEN_ENTROPY\", \"ENTROPY\", \"BALANCED\", \"RANDOM\", \"KMEANS_RANDOM\", \"KMEANS_REPRESENTATIVE\"]\n",
    "REPEAT_SAMPLING = 5 #@param {type:'slider', min:1, max:10, step:1}\n",
    "# @markdown &emsp; ↳ Repeat the experiment with different sampling seeds ∊ [0, REPEAT_SAMPLING)\n",
    "KMEANS_INIT = 'random' #@param [\"k-means++\", \"random\"]\n",
    "KMEANS_CENTROID_SEEDS = 10 #@param {type:'slider', min:1, max:10, step:1}\n",
    "KMEANS_ALGORITHM = 'lloyd' #@param [\"lloyd\", \"elkan\"]\n",
    "UNREP_ALPHA = 10\n",
    "\n",
    "RESET_MODEL_AFTER_EACH_ITERATION = True #@param {type:\"boolean\"}\n",
    "PREDICT_WITH_GENERATE = True #@param {type:\"boolean\"}\n",
    "STORE_RESULTS = [] # Array of metrics' names: save and store results of a suite of notebooks\n",
    "# ↳ Example: ['eval_main_metric', 'eval_agnews_macro_f1']\n",
    "SHOW_VAL_PREDICTIONS = True #@param {type:\"boolean\"}\n",
    "MAX_FOLDS = 1\n",
    "\n",
    "USE_CONTRASTIVE_LOSS = True #@param {type:\"boolean\"}\n",
    "CONTRASTIVE_LOSS_ALPHA = 0.01 #@param {type:\"number\"}\n",
    "SIMILARITY_METRIC = 'cosine' #@param [\"cosine\", \"euclidean\"]\n",
    "TEMPERATURE = 0.1 #@param {type:\"number\"}\n",
    "USE_HARD_NEGATIVE_MINING = True #@param {type:\"boolean\"}\n",
    "HARD_NEGATIVE_MINING_M = 5 #@param {type:\"integer\"}\n",
    "\n",
    "# Checkpoints\n",
    "SKIP_SAMPLING_SEEDS = 0 # Skip the first few seeds if things went wrong during the previous runtime.\n",
    "RESTORE_CHECKPOINT_FROM_FILE = True #@param {type:\"boolean\"}\n",
    "# If restore from file was false, use the following checkpoint variables:\n",
    "INIT_FOLDS_VAL_LOG = [[]]\n",
    "INIT_FOLDS_TEST_LOG = [[]]\n",
    "INIT_PREDS_TEXTS_VAL_LIST = [[]]\n",
    "INIT_TARGETS_TEXTS_VAL_LIST = [ ]\n",
    "INIT_SAMPLED_LIST = [[]]\n",
    "\n",
    "# IDs\n",
    "TYPE_IDS = ['agreement', 'argue', 'intention', 'sentiment']\n",
    "POLARITY_IDS = ['negative', 'positive']\n",
    "INTENSITY_IDS = ['slight', 'low', 'medium', 'high', 'extreme']\n",
    "AGNEWS_IDS = ['world', 'sports', 'business', 'science']\n",
    "AMZN_IDS = ['1', '2', '3', '4', '5']\n",
    "\n",
    "# @markdown <br/> <h3> Dataset Links </h3>\n",
    "# @markdown *(Either URLs or local file paths)* <br/>\n",
    "\n",
    "# @markdown **MPQA Type**\n",
    "DATA_ROOT = './data'\n",
    "mpqa_t_train_link = f'{DATA_ROOT}/MPQA_dataset/mpqa_type_train.csv' #@param {type: \"string\"}\n",
    "mpqa_t_val_link   = f'{DATA_ROOT}/MPQA_dataset/mpqa_type_val.csv' #@param {type: \"string\"}\n",
    "mpqa_t_test_link  = f'{DATA_ROOT}/MPQA_dataset/mpqa_type_test.csv' #@param {type: \"string\"}\n",
    "\n",
    "# @markdown **MPQA Polarity**\n",
    "mpqa_p_train_link = f'{DATA_ROOT}/MPQA_dataset/mpqa_polarity_train.csv' #@param {type: \"string\"}\n",
    "mpqa_p_val_link   = f'{DATA_ROOT}/MPQA_dataset/mpqa_polarity_val.csv' #@param {type: \"string\"}\n",
    "mpqa_p_test_link  = f'{DATA_ROOT}/MPQA_dataset/mpqa_polarity_test.csv' #@param {type: \"string\"}\n",
    "\n",
    "# @markdown **MPQA Intensity**\n",
    "mpqa_i_train_link = f'{DATA_ROOT}/MPQA_dataset/mpqa_intensity_train.csv' #@param {type: \"string\"}\n",
    "mpqa_i_val_link   = f'{DATA_ROOT}/MPQA_dataset/mpqa_intensity_val.csv' #@param {type: \"string\"}\n",
    "mpqa_i_test_link  = f'{DATA_ROOT}/MPQA_dataset/mpqa_intensity_test.csv' #@param {type: \"string\"}\n",
    "\n",
    "# @markdown **AG News**\n",
    "agnews_train_link = f'{DATA_ROOT}/AG_news_dataset/agnews_train.csv' #@param {type: \"string\"}\n",
    "agnews_test_link  = f'{DATA_ROOT}/AG_news_dataset/agnews_test.csv' #@param {type: \"string\"}\n",
    "\n",
    "# @markdown **Amazon Reviews**\n",
    "amzn_train_link = f'{DATA_ROOT}/Amazon_dataset/amazon_train.csv' #@param {type: \"string\"}\n",
    "amzn_val_link   = f'{DATA_ROOT}/Amazon_dataset/amazon_val.csv' #@param {type: \"string\"}\n",
    "amzn_test_link  = f'{DATA_ROOT}/Amazon_dataset/amazon_test.csv' #@param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XWcThH1lCtNx",
   "metadata": {
    "id": "XWcThH1lCtNx"
   },
   "outputs": [],
   "source": [
    "NUMBER_OF_SAMPLES_PER_ITERATION = [NUMBER_OF_SAMPLES_PER_ITERATION] * SAMPLING_ITERATIONS if type(NUMBER_OF_SAMPLES_PER_ITERATION).__name__ != 'list' else NUMBER_OF_SAMPLES_PER_ITERATION # Can be a list too; the length of the list should be equal to the number of iterations\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = min(PER_DEVICE_TRAIN_BATCH_SIZE, min(NUMBER_OF_SAMPLES_PER_ITERATION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8doff7pY3ekf",
   "metadata": {
    "id": "8doff7pY3ekf"
   },
   "outputs": [],
   "source": [
    "# Set the parameters that can be derived from the main parameters\n",
    "\n",
    "if BASE_MODEL == 'BART':\n",
    "    MODEL_NAME = 'facebook/bart-base'\n",
    "    LEARNING_RATE = 5e-5\n",
    "elif BASE_MODEL == 'FLAN-T5':\n",
    "    MODEL_NAME = 'google/flan-t5-base'\n",
    "    LEARNING_RATE = 1e-4\n",
    "\n",
    "if AFL_METHOD == 'Random':\n",
    "    SAMPLING_METHOD_FIRST_ITERATION = 'RANDOM'\n",
    "    # NUMBER_OF_SAMPLES_PER_ITERATION *= SAMPLING_ITERATIONS\n",
    "    # SAMPLING_ITERATIONS = 1\n",
    "\n",
    "elif AFL_METHOD == 'Rep(En)':\n",
    "    if BASE_MODEL == 'BART':\n",
    "        EMBEDDING_METHOD_FIRST_ITERATION = 'BART-ENCODER'\n",
    "        SAMPLING_METHOD_FIRST_ITERATION = 'KMEANS_REPRESENTATIVE'\n",
    "        NUMBER_OF_SAMPLES_PER_ITERATION *= SAMPLING_ITERATIONS\n",
    "        SAMPLING_ITERATIONS = 1\n",
    "    elif BASE_MODEL == 'FLAN-T5':\n",
    "        EMBEDDING_METHOD_FIRST_ITERATION = 'FLAN-T5-ENCODER'\n",
    "        SAMPLING_METHOD_FIRST_ITERATION = 'KMEANS_REPRESENTATIVE'\n",
    "        NUMBER_OF_SAMPLES_PER_ITERATION *= SAMPLING_ITERATIONS\n",
    "        SAMPLING_ITERATIONS = 1\n",
    "\n",
    "elif AFL_METHOD == 'Rep(En)-Un':\n",
    "    if BASE_MODEL == 'BART':\n",
    "        EMBEDDING_METHOD_FIRST_ITERATION = 'BART-ENCODER'\n",
    "        EMBEDDING_METHOD_SECOND_ITERATION_PLUS = 'BART-SCORES'\n",
    "        SAMPLING_METHOD_FIRST_ITERATION = 'KMEANS_REPRESENTATIVE'\n",
    "        SAMPLING_METHOD_SECOND_ITERATION_PLUS = 'ENTROPY'\n",
    "    elif BASE_MODEL == 'FLAN-T5':\n",
    "        EMBEDDING_METHOD_FIRST_ITERATION = 'FLAN-T5-ENCODER'\n",
    "        EMBEDDING_METHOD_SECOND_ITERATION_PLUS = 'FLAN-T5-SCORES'\n",
    "        SAMPLING_METHOD_FIRST_ITERATION = 'KMEANS_REPRESENTATIVE'\n",
    "        SAMPLING_METHOD_SECOND_ITERATION_PLUS = 'ENTROPY'\n",
    "\n",
    "elif AFL_METHOD == 'Rep(En)-Rep(Sc)':\n",
    "    if BASE_MODEL == 'BART':\n",
    "        EMBEDDING_METHOD_FIRST_ITERATION = 'BART-ENCODER'\n",
    "        EMBEDDING_METHOD_SECOND_ITERATION_PLUS = 'BART-SCORES'\n",
    "        SAMPLING_METHOD_FIRST_ITERATION = 'KMEANS_REPRESENTATIVE'\n",
    "        SAMPLING_METHOD_SECOND_ITERATION_PLUS = 'KMEANS_REPRESENTATIVE'\n",
    "    elif BASE_MODEL == 'FLAN-T5':\n",
    "        EMBEDDING_METHOD_FIRST_ITERATION = 'FLAN-T5-ENCODER'\n",
    "        EMBEDDING_METHOD_SECOND_ITERATION_PLUS = 'FLAN-T5-SCORES'\n",
    "        SAMPLING_METHOD_FIRST_ITERATION = 'KMEANS_REPRESENTATIVE'\n",
    "        SAMPLING_METHOD_SECOND_ITERATION_PLUS = 'KMEANS_REPRESENTATIVE'\n",
    "\n",
    "elif AFL_METHOD == 'Rep(En)-Rep(En)':\n",
    "    if BASE_MODEL == 'BART':\n",
    "        EMBEDDING_METHOD_FIRST_ITERATION = 'BART-ENCODER'\n",
    "        EMBEDDING_METHOD_SECOND_ITERATION_PLUS = 'BART-ENCODER'\n",
    "        SAMPLING_METHOD_FIRST_ITERATION = 'KMEANS_REPRESENTATIVE'\n",
    "        SAMPLING_METHOD_SECOND_ITERATION_PLUS = 'KMEANS_REPRESENTATIVE'\n",
    "    elif BASE_MODEL == 'FLAN-T5':\n",
    "        EMBEDDING_METHOD_FIRST_ITERATION = 'FLAN-T5-ENCODER'\n",
    "        EMBEDDING_METHOD_SECOND_ITERATION_PLUS = 'FLAN-T5-ENCODER'\n",
    "        SAMPLING_METHOD_FIRST_ITERATION = 'KMEANS_REPRESENTATIVE'\n",
    "        SAMPLING_METHOD_SECOND_ITERATION_PLUS = 'KMEANS_REPRESENTATIVE'\n",
    "\n",
    "elif AFL_METHOD == 'Rep(En)-UnRep':\n",
    "    if BASE_MODEL == 'BART':\n",
    "        EMBEDDING_METHOD_FIRST_ITERATION = 'BART-ENCODER'\n",
    "        EMBEDDING_METHOD_SECOND_ITERATION_PLUS = 'BART-SCORES+ENCODER'\n",
    "        SAMPLING_METHOD_FIRST_ITERATION = 'KMEANS_REPRESENTATIVE'\n",
    "        SAMPLING_METHOD_SECOND_ITERATION_PLUS = 'ENTROPY_THEN_KMEANS_REPRESENTATIVE'\n",
    "    elif BASE_MODEL == 'FLAN-T5':\n",
    "        EMBEDDING_METHOD_FIRST_ITERATION = 'FLAN-T5-ENCODER'\n",
    "        EMBEDDING_METHOD_SECOND_ITERATION_PLUS = 'FLAN-T5-SCORES+ENCODER'\n",
    "        SAMPLING_METHOD_FIRST_ITERATION = 'KMEANS_REPRESENTATIVE'\n",
    "        SAMPLING_METHOD_SECOND_ITERATION_PLUS = 'ENTROPY_THEN_KMEANS_REPRESENTATIVE'\n",
    "\n",
    "elif AFL_METHOD == 'Rep(En)-ClUn(Sc)':\n",
    "    if BASE_MODEL == 'BART':\n",
    "        EMBEDDING_METHOD_FIRST_ITERATION = 'BART-ENCODER'\n",
    "        EMBEDDING_METHOD_SECOND_ITERATION_PLUS = 'BART-SCORES'\n",
    "        SAMPLING_METHOD_FIRST_ITERATION = 'KMEANS_REPRESENTATIVE'\n",
    "        SAMPLING_METHOD_SECOND_ITERATION_PLUS = 'KMEANS_REPRESENTATIVE_THEN_ENTROPY'\n",
    "    elif BASE_MODEL == 'FLAN-T5':\n",
    "        EMBEDDING_METHOD_FIRST_ITERATION = 'FLAN-T5-ENCODER'\n",
    "        EMBEDDING_METHOD_SECOND_ITERATION_PLUS = 'FLAN-T5-SCORES'\n",
    "        SAMPLING_METHOD_FIRST_ITERATION = 'KMEANS_REPRESENTATIVE'\n",
    "        SAMPLING_METHOD_SECOND_ITERATION_PLUS = 'KMEANS_REPRESENTATIVE_THEN_ENTROPY'\n",
    "\n",
    "elif AFL_METHOD == 'Rep(En)-ClUn(En)':\n",
    "    if BASE_MODEL == 'BART':\n",
    "        EMBEDDING_METHOD_FIRST_ITERATION = 'BART-ENCODER'\n",
    "        EMBEDDING_METHOD_SECOND_ITERATION_PLUS = 'BART-SCORES+ENCODER'\n",
    "        SAMPLING_METHOD_FIRST_ITERATION = 'KMEANS_REPRESENTATIVE'\n",
    "        SAMPLING_METHOD_SECOND_ITERATION_PLUS = 'KMEANS_REPRESENTATIVE_THEN_ENTROPY'\n",
    "    elif BASE_MODEL == 'FLAN-T5':\n",
    "        EMBEDDING_METHOD_FIRST_ITERATION = 'FLAN-T5-ENCODER'\n",
    "        EMBEDDING_METHOD_SECOND_ITERATION_PLUS = 'FLAN-T5-SCORES+ENCODER'\n",
    "        SAMPLING_METHOD_FIRST_ITERATION = 'KMEANS_REPRESENTATIVE'\n",
    "        SAMPLING_METHOD_SECOND_ITERATION_PLUS = 'KMEANS_REPRESENTATIVE_THEN_ENTROPY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce192a0a-9703-4393-ab55-c24cdd1222cc",
   "metadata": {
    "id": "ce192a0a-9703-4393-ab55-c24cdd1222cc"
   },
   "outputs": [],
   "source": [
    "if RUNTIME_TYPE == 'COLAB':\n",
    "    %pip install transformers==4.25.1 datasets --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xyDbVHftZ0it",
   "metadata": {
    "id": "xyDbVHftZ0it"
   },
   "outputs": [],
   "source": [
    "# To assure deterministic results\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":4096:8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f96831-ae2c-4adc-8730-cc07b9b5ba74",
   "metadata": {
    "id": "96f96831-ae2c-4adc-8730-cc07b9b5ba74",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import itertools\n",
    "import functools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, EarlyStoppingCallback, AutoConfig\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, pairwise_distances_argmin_min, confusion_matrix, multilabel_confusion_matrix\n",
    "from sklearn.utils.random import sample_without_replacement\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from scipy.special import softmax\n",
    "from urllib.request import urlopen\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import statistics\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from functools import wraps\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WHtALgYKZzhP",
   "metadata": {
    "id": "WHtALgYKZzhP"
   },
   "outputs": [],
   "source": [
    "# Start timer\n",
    "\n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Lv87oZj_IXcE",
   "metadata": {
    "id": "Lv87oZj_IXcE"
   },
   "outputs": [],
   "source": [
    "# Support for third-party widgets\n",
    "\n",
    "if RUNTIME_TYPE == 'COLAB':\n",
    "    from google.colab import output\n",
    "    output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2ab8c6-603d-40af-875b-5b6f68f9accc",
   "metadata": {
    "id": "2d2ab8c6-603d-40af-875b-5b6f68f9accc"
   },
   "outputs": [],
   "source": [
    "# Setup device\n",
    "\n",
    "device_string = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device_hf = 0 if torch.cuda.is_available() else -1\n",
    "device = torch.device(device_string)\n",
    "print(\"Device:\", device)\n",
    "NUM_WORKERS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856a085e-a728-4a8e-8f75-193820216f52",
   "metadata": {
    "id": "856a085e-a728-4a8e-8f75-193820216f52"
   },
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "\n",
    "CALLBACKS = []\n",
    "if EARLY_STOPPING > 0:\n",
    "    CALLBACKS.append(EarlyStoppingCallback(EARLY_STOPPING))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a21171-f03f-45d1-9293-4bcb4183eb02",
   "metadata": {
    "id": "b6a21171-f03f-45d1-9293-4bcb4183eb02"
   },
   "outputs": [],
   "source": [
    "def set_seed():\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8vCH651D1kmE",
   "metadata": {
    "id": "8vCH651D1kmE"
   },
   "source": [
    "# Restore Last Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8nMi6Jju1i9T",
   "metadata": {
    "id": "8nMi6Jju1i9T"
   },
   "outputs": [],
   "source": [
    "if RESTORE_CHECKPOINT_FROM_FILE:\n",
    "    try:\n",
    "        with open(f'results/{EXPERIMENT_NAME.replace(\".ipynb\", \"\")}_checkpoint.json') as checkpoint_file:\n",
    "            checkpoint = json.load(checkpoint_file)\n",
    "            INIT_FOLDS_VAL_LOG = checkpoint['INIT_FOLDS_VAL_LOG']\n",
    "            INIT_FOLDS_TEST_LOG = checkpoint['INIT_FOLDS_TEST_LOG']\n",
    "            INIT_PREDS_TEXTS_VAL_LIST = checkpoint['INIT_PREDS_TEXTS_VAL_LIST']\n",
    "            INIT_SAMPLED_LIST = checkpoint['INIT_SAMPLED_LIST']\n",
    "            SKIP_SAMPLING_SEEDS = len(INIT_FOLDS_VAL_LOG[-1])\n",
    "            print(f\"Skipped the first {SKIP_SAMPLING_SEEDS} seeds.\")\n",
    "    except Exception as e:\n",
    "        None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65615ad0-5c82-4079-be75-122e75d8441c",
   "metadata": {
    "id": "65615ad0-5c82-4079-be75-122e75d8441c"
   },
   "source": [
    "# Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JgkEqLVciHXV",
   "metadata": {
    "id": "JgkEqLVciHXV"
   },
   "outputs": [],
   "source": [
    "# Classes\n",
    "\n",
    "TYPE_CLASSES = ['agreement', 'arguing', 'intention', 'sentiment']\n",
    "POLARITY_CLASSES = ['negative', 'positive']\n",
    "INTENSITY_CLASSES = ['low', 'low medium', 'medium', 'medium high', 'high']\n",
    "AGNEWS_CLASSES = [1, 2, 3, 4]\n",
    "AMZN_CLASSES = [1, 2, 3, 4, 5]\n",
    "\n",
    "NUM_TYPE_CLASSES = len(TYPE_CLASSES)\n",
    "NUM_POLARITY_CLASSES = len(POLARITY_CLASSES)\n",
    "NUM_INTENSITY_CLASSES = 5\n",
    "NUM_INTENSITY_UNITS = 3\n",
    "NUM_AGNEWS_CLASSES = len(AGNEWS_CLASSES)\n",
    "NUM_AMZN_CLASSES = len(AMZN_CLASSES)\n",
    "\n",
    "if DATASET_NAME == 'MPQA-T':\n",
    "    CLASSES = TYPE_CLASSES\n",
    "    NUM_CLASSES = NUM_TYPE_CLASSES\n",
    "    CLASS_IDS = TYPE_IDS\n",
    "elif DATASET_NAME == 'MPQA-P':\n",
    "    CLASSES = POLARITY_CLASSES\n",
    "    NUM_CLASSES = NUM_POLARITY_CLASSES\n",
    "    CLASS_IDS = POLARITY_IDS\n",
    "elif DATASET_NAME == 'MPQA-I':\n",
    "    CLASSES = INTENSITY_CLASSES\n",
    "    NUM_CLASSES = NUM_INTENSITY_CLASSES\n",
    "    CLASS_IDS = INTENSITY_IDS\n",
    "elif DATASET_NAME == 'AGNEWS':\n",
    "    CLASSES = AGNEWS_CLASSES\n",
    "    NUM_CLASSES = NUM_AGNEWS_CLASSES\n",
    "    CLASS_IDS = AGNEWS_IDS\n",
    "elif DATASET_NAME.startswith('AMZN'):\n",
    "    CLASSES = AMZN_CLASSES\n",
    "    NUM_CLASSES = NUM_AMZN_CLASSES\n",
    "    CLASS_IDS = AMZN_IDS\n",
    "else:\n",
    "    CLASSES = []\n",
    "    NUM_CLASSES = 0\n",
    "    CLASS_IDS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o-yP-vC9iOfm",
   "metadata": {
    "id": "o-yP-vC9iOfm"
   },
   "outputs": [],
   "source": [
    "# Create a map for class ids and class names\n",
    "\n",
    "type_classname2classindex = {\n",
    "    'agreement': 0,\n",
    "    'arguing':   1,\n",
    "    'intention': 2,\n",
    "    'sentiment': 3,\n",
    "}\n",
    "type_classname2classid = {\n",
    "    'agreement': TYPE_IDS[0],\n",
    "    'arguing':   TYPE_IDS[1],\n",
    "    'intention': TYPE_IDS[2],\n",
    "    'sentiment': TYPE_IDS[3],\n",
    "}\n",
    "type_classid2classname = {v:k for k, v in type_classname2classid.items()}\n",
    "type_classid2classindex = {type_classname2classid[k]:v for k, v in type_classname2classindex.items()}\n",
    "\n",
    "###\n",
    "\n",
    "polarity_classname2classindex = {\n",
    "    'negative': 0,\n",
    "    'positive': 1,\n",
    "}\n",
    "polarity_classname2classid = {\n",
    "    'negative': POLARITY_IDS[0],\n",
    "    'positive': POLARITY_IDS[1],\n",
    "}\n",
    "polarity_classid2classname = {v:k for k, v in polarity_classname2classid.items()}\n",
    "polarity_classid2classindex = {polarity_classname2classid[k]:v for k, v in polarity_classname2classindex.items()}\n",
    "\n",
    "###\n",
    "\n",
    "intensity_classname2classindices = {\n",
    "    'low':        [0],\n",
    "    'low medium': [0, 1],\n",
    "    'medium':        [1],\n",
    "    'medium high':   [1, 2],\n",
    "    'high':             [2],\n",
    "}\n",
    "intensity_classname2classid = {\n",
    "    'low':         INTENSITY_IDS[0],\n",
    "    'low medium':  INTENSITY_IDS[1],\n",
    "    'medium':      INTENSITY_IDS[2],\n",
    "    'medium high': INTENSITY_IDS[3],\n",
    "    'high':        INTENSITY_IDS[4],\n",
    "}\n",
    "intensity_classid2classname = {v:k for k, v in intensity_classname2classid.items()}\n",
    "intensity_classid2classindices = {intensity_classname2classid[k]:v for k, v in intensity_classname2classindices.items()}\n",
    "\n",
    "###\n",
    "\n",
    "agnews_classname2classindex = {\n",
    "    1: 0,\n",
    "    2: 1,\n",
    "    3: 2,\n",
    "    4: 3,\n",
    "}\n",
    "agnews_classname2classid = {\n",
    "    1: AGNEWS_IDS[0],\n",
    "    2: AGNEWS_IDS[1],\n",
    "    3: AGNEWS_IDS[2],\n",
    "    4: AGNEWS_IDS[3],\n",
    "}\n",
    "agnews_classid2classname = {v:k for k, v in agnews_classname2classid.items()}\n",
    "agnews_classid2classindex = {agnews_classname2classid[k]:v for k, v in agnews_classname2classindex.items()}\n",
    "\n",
    "###\n",
    "\n",
    "amzn_classname2classindex = {\n",
    "    1: 0,\n",
    "    2: 1,\n",
    "    3: 2,\n",
    "    4: 3,\n",
    "    5: 4,\n",
    "}\n",
    "amzn_classname2classid = {\n",
    "    1: AMZN_IDS[0],\n",
    "    2: AMZN_IDS[1],\n",
    "    3: AMZN_IDS[2],\n",
    "    4: AMZN_IDS[3],\n",
    "    5: AMZN_IDS[4],\n",
    "}\n",
    "amzn_classid2classname = {v:k for k, v in amzn_classname2classid.items()}\n",
    "amzn_classid2classindex = {amzn_classname2classid[k]:v for k, v in amzn_classname2classindex.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oMj4Vtcst4Fv",
   "metadata": {
    "id": "oMj4Vtcst4Fv"
   },
   "outputs": [],
   "source": [
    "# Labels Distribution\n",
    "\n",
    "def count_labels(Y=None, Y_train=None, Y_val=None, Y_test=None):\n",
    "\n",
    "    if Y==None:\n",
    "\n",
    "        count_dict = {}\n",
    "        for y in Y_train:\n",
    "            count_dict[y] = count_dict.get(y, 0) + 1\n",
    "        print('Train Labels distribution:', count_dict)\n",
    "\n",
    "        count_dict = {}\n",
    "        for y in Y_val:\n",
    "            count_dict[y] = count_dict.get(y, 0) + 1\n",
    "        print('Validation Labels distribution:', count_dict)\n",
    "\n",
    "        count_dict = {}\n",
    "        for y in Y_test:\n",
    "            count_dict[y] = count_dict.get(y, 0) + 1\n",
    "        print('Test Labels distribution:', count_dict)\n",
    "\n",
    "        Y = Y_train + Y_val + Y_test\n",
    "\n",
    "    count_dict = {}\n",
    "    for y in Y:\n",
    "        count_dict[y] = count_dict.get(y, 0) + 1\n",
    "    print('Labels distribution:', count_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fVJr8C-6U-YA",
   "metadata": {
    "id": "fVJr8C-6U-YA"
   },
   "outputs": [],
   "source": [
    "# Decompose X and y\n",
    "\n",
    "def decompose_mpqa(dataset):\n",
    "    if AFL_APPROACH == \"FINE-TUNING\":\n",
    "        X = [i for i in dataset.input]\n",
    "    elif AFL_APPROACH == \"IN-CONTEXT\":\n",
    "        X = ['{ ' + f'{\" \".join(i.splitlines())}' + ' } :' for i in dataset.input]\n",
    "\n",
    "    if DATASET_NAME == 'MPQA-T':\n",
    "        y = [i.replace(TYPE_CLASSES[0], TYPE_IDS[0]) \\\n",
    "              .replace(TYPE_CLASSES[1], TYPE_IDS[1]) \\\n",
    "              .replace(TYPE_CLASSES[2], TYPE_IDS[2]) \\\n",
    "              .replace(TYPE_CLASSES[3], TYPE_IDS[3]) for i in dataset.output]\n",
    "    elif DATASET_NAME == 'MPQA-P':\n",
    "        y = [i.replace(POLARITY_CLASSES[0], POLARITY_IDS[0]) \\\n",
    "              .replace(POLARITY_CLASSES[1], POLARITY_IDS[1]) for i in dataset.output]\n",
    "    elif DATASET_NAME == 'MPQA-I':\n",
    "        # Add special characters to avoid conflicts\n",
    "        y = [('!'+i+'!').replace('!'+INTENSITY_CLASSES[0]+'!', INTENSITY_IDS[0]) \\\n",
    "                        .replace('!'+INTENSITY_CLASSES[1]+'!', INTENSITY_IDS[1]) \\\n",
    "                        .replace('!'+INTENSITY_CLASSES[2]+'!', INTENSITY_IDS[2]) \\\n",
    "                        .replace('!'+INTENSITY_CLASSES[3]+'!', INTENSITY_IDS[3]) \\\n",
    "                        .replace('!'+INTENSITY_CLASSES[4]+'!', INTENSITY_IDS[4]) for i in dataset.output]\n",
    "    return X, y\n",
    "\n",
    "def decompose_agnews(dataset):\n",
    "    if AFL_APPROACH == \"FINE-TUNING\":\n",
    "        X = [f'{title} | {desc}' for title, desc in zip(dataset.Title, dataset.Description)]\n",
    "    elif AFL_APPROACH == \"IN-CONTEXT\":\n",
    "        X = ['{ ' + f'{\" \".join(title.splitlines())} | {\" \".join(desc.splitlines())}' + ' } :' for title, desc in zip(dataset.Title, dataset.Description)]\n",
    "    y = [AGNEWS_IDS[i-1] for i in dataset['Class Index']]\n",
    "    return X, y\n",
    "\n",
    "def decompose_amzn(dataset, filter_lang='en'):\n",
    "    X = []\n",
    "    y = []\n",
    "    for title, body, lang, stars in zip(dataset.review_title, dataset.review_body, dataset.language, dataset.stars):\n",
    "        if lang == filter_lang:\n",
    "            if AFL_APPROACH == \"FINE-TUNING\":\n",
    "                X = X + [f'{title} | {body}'[:900]]\n",
    "            elif AFL_APPROACH == \"IN-CONTEXT\":\n",
    "                X = X + ['{ ' + f'{\" \".join(title.splitlines())} | {\" \".join(body.splitlines())}'[:900] + ' } :']\n",
    "            y = y + [AMZN_IDS[stars-1]]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RA2RdjfcZ4Lq",
   "metadata": {
    "id": "RA2RdjfcZ4Lq"
   },
   "outputs": [],
   "source": [
    "@functools.cache\n",
    "def pd_read_csv(link):\n",
    "    if link.startswith(\"http://\") or link.startswith(\"https://\"):\n",
    "        # Handle URLs\n",
    "        t = 15\n",
    "        while True:\n",
    "            try:\n",
    "                return pd.read_csv(link)\n",
    "            except Exception as e:\n",
    "                print(f\"Unsuccessful attempt to download {link}. Waiting for {t}s.\")\n",
    "                time.sleep(t)\n",
    "                t *= random.random()+1\n",
    "                t = int(t)\n",
    "                continue\n",
    "    else:\n",
    "        # Handle local file paths\n",
    "        return pd.read_csv(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28384faa-36ff-4caa-b598-4946d990c7ba",
   "metadata": {
    "id": "28384faa-36ff-4caa-b598-4946d990c7ba"
   },
   "outputs": [],
   "source": [
    "# Fetch the dataset\n",
    "\n",
    "def fetch_dataset(dataset_name=DATASET_NAME, fold=0, show_counter=False):\n",
    "\n",
    "    if dataset_name.startswith('MPQA-'):\n",
    "\n",
    "        if dataset_name == 'MPQA-T':\n",
    "            trainset = pd_read_csv(mpqa_t_train_link)\n",
    "            valset   = pd_read_csv(mpqa_t_val_link)\n",
    "            testset  = pd_read_csv(mpqa_t_test_link)\n",
    "        elif dataset_name == 'MPQA-P':\n",
    "            trainset = pd_read_csv(mpqa_p_train_link)\n",
    "            valset   = pd_read_csv(mpqa_p_val_link)\n",
    "            testset  = pd_read_csv(mpqa_p_test_link)\n",
    "        elif dataset_name == 'MPQA-I':\n",
    "            trainset = pd_read_csv(mpqa_i_train_link)\n",
    "            valset   = pd_read_csv(mpqa_i_val_link)\n",
    "            testset  = pd_read_csv(mpqa_i_test_link)\n",
    "\n",
    "        X_train, y_train = decompose_mpqa(trainset)\n",
    "        X_val,   y_val   = decompose_mpqa(valset)\n",
    "        X_test,  y_test  = decompose_mpqa(testset)\n",
    "\n",
    "        if show_counter == True:\n",
    "            count_labels(Y_train=y_train, Y_val=y_val, Y_test=y_test)\n",
    "\n",
    "    elif dataset_name == 'AGNEWS':\n",
    "\n",
    "        trainvalset = pd_read_csv(agnews_trainval_link)\n",
    "        testset     = pd_read_csv(agnews_test_link)\n",
    "\n",
    "        X_train_val, y_train_val = decompose_agnews(trainvalset)\n",
    "        X_test,      y_test      = decompose_agnews(testset)\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=1200, shuffle=True, stratify=y_train_val, random_state=SEED)\n",
    "\n",
    "        if show_counter == True:\n",
    "            count_labels(Y_train=y_train, Y_val=y_val, Y_test=y_test)\n",
    "\n",
    "    elif dataset_name.startswith('AMZN'):\n",
    "\n",
    "        filter_lang = dataset_name[-2:].lower()\n",
    "\n",
    "        trainset = pd_read_csv(amzn_train_link)\n",
    "        valset   = pd_read_csv(amzn_val_link)\n",
    "        testset  = pd_read_csv(amzn_test_link)\n",
    "\n",
    "        X_train, y_train = decompose_amzn(trainset, filter_lang)\n",
    "        X_val,   y_val   = decompose_amzn(valset,   filter_lang)\n",
    "        X_test,  y_test  = decompose_amzn(testset,  filter_lang)\n",
    "\n",
    "        _, X_val, _, y_val = train_test_split(X_val, y_val, test_size=1200, shuffle=True, stratify=y_val, random_state=SEED)\n",
    "\n",
    "        if show_counter == True:\n",
    "            count_labels(Y_train=y_train, Y_val=y_val, Y_test=y_test)\n",
    "\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee725191-73e1-47ee-a8d6-1465aa32b6f4",
   "metadata": {
    "id": "ee725191-73e1-47ee-a8d6-1465aa32b6f4"
   },
   "outputs": [],
   "source": [
    "# All samples\n",
    "\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test) = fetch_dataset()\n",
    "\n",
    "print(f'First three samples (out of {len(y_train)+len(y_val)+len(y_test)} = {len(y_train)} + {len(y_val)} + {len(y_test)}):')\n",
    "for i in range(0, 3):\n",
    "    print('┌X:', X_train[i])\n",
    "    print('└y:', y_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ab1607-bad3-4222-b15c-0ac3ec39acd5",
   "metadata": {
    "id": "37ab1607-bad3-4222-b15c-0ac3ec39acd5"
   },
   "source": [
    "# Preparing the model and torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b732e3ad-d4f0-4ee5-8d6c-0bdc523539f8",
   "metadata": {
    "id": "b732e3ad-d4f0-4ee5-8d6c-0bdc523539f8"
   },
   "outputs": [],
   "source": [
    "def load_tokenizer(model_name=MODEL_NAME):\n",
    "    return AutoTokenizer.from_pretrained(model_name, model_max_length=512, local_files_only=True)\n",
    "\n",
    "def load_model(model_name=MODEL_NAME):\n",
    "    config = AutoConfig.from_pretrained(model_name, local_files_only=True)\n",
    "    \n",
    "    config.num_labels = NUM_CLASSES \n",
    "    \n",
    "    if config.is_encoder_decoder:\n",
    "        model_hf = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_name, config=config, local_files_only=True\n",
    "        )\n",
    "    else:\n",
    "        from transformers import AutoModelForSequenceClassification\n",
    "        model_hf = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, config=config, local_files_only=True\n",
    "        )\n",
    "        \n",
    "    pytorch_total_params = sum(p.numel() for p in model_hf.parameters() if p.requires_grad)\n",
    "    print('Number of trainable parameters in base model:', pytorch_total_params)\n",
    "    \n",
    "    return model_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Bga6t-ZBdJlN",
   "metadata": {
    "id": "Bga6t-ZBdJlN"
   },
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer()\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g9_I4PiZrNS2",
   "metadata": {
    "id": "g9_I4PiZrNS2"
   },
   "outputs": [],
   "source": [
    "# Show how each class is tokenized\n",
    "\n",
    "for class_id in CLASS_IDS:\n",
    "    print(f'{class_id}: {tokenizer(class_id)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1odOETprA-P",
   "metadata": {
    "id": "b1odOETprA-P"
   },
   "outputs": [],
   "source": [
    "# Store the first token only\n",
    "if 'flan-t5' in MODEL_NAME.lower():\n",
    "    CLASSES_TOKENS = [tokenizer(class_id).input_ids[0] for class_id in CLASS_IDS]\n",
    "elif 'bart' in MODEL_NAME.lower():\n",
    "    CLASSES_TOKENS = [tokenizer(class_id).input_ids[1] for class_id in CLASS_IDS]\n",
    "elif 'roberta' in MODEL_NAME.lower():\n",
    "    CLASSES_TOKENS = [tokenizer(class_id).input_ids[1] for class_id in CLASS_IDS]\n",
    "else:\n",
    "    raise Exception('The behavior for new tokenizer needs to be configured.')\n",
    "print(CLASSES_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psC9urXWVUGm",
   "metadata": {
    "id": "psC9urXWVUGm"
   },
   "outputs": [],
   "source": [
    "if EMBEDDING_METHOD_FIRST_ITERATION in ['BART-ENCODER']:\n",
    "    emb_tokenizer = load_tokenizer(MODEL_NAME if 'bart' in MODEL_NAME.lower() else 'facebook/bart-base')\n",
    "if EMBEDDING_METHOD_FIRST_ITERATION in ['T5-ENCODER']:\n",
    "    emb_tokenizer = load_tokenizer(MODEL_NAME if 't5' in MODEL_NAME.lower() else 't5-base')\n",
    "if EMBEDDING_METHOD_FIRST_ITERATION in ['FLAN-T5-ENCODER']:\n",
    "    if 'roberta' in MODEL_NAME.lower():\n",
    "         emb_tokenizer = load_tokenizer(MODEL_NAME)\n",
    "    else:\n",
    "         emb_tokenizer = load_tokenizer('google/flan-t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gPKlVbH-rHkl",
   "metadata": {
    "id": "gPKlVbH-rHkl"
   },
   "outputs": [],
   "source": [
    "# Load optimizer\n",
    "\n",
    "def load_optimizer(model):\n",
    "    opt_parameters = model.parameters()\n",
    "    return torch.optim.AdamW(opt_parameters, lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ceb2ae-6206-4e83-92b7-46b7a22c93d0",
   "metadata": {
    "id": "33ceb2ae-6206-4e83-92b7-46b7a22c93d0"
   },
   "outputs": [],
   "source": [
    "# Tokenize the inputs\n",
    "\n",
    "@functools.cache\n",
    "def tokenize_inputs(tokenizer, X, y):\n",
    "    if type(X) is tuple:\n",
    "        X = list(X)\n",
    "        y = list(y)\n",
    "\n",
    "    Xy_tokenized = tokenizer(X, text_target=y, truncation=True)\n",
    "    return Xy_tokenized\n",
    "\n",
    "def tokenize_and_show_inputs(tokenizer, X='', y=''):\n",
    "    print(\"\\nPretokenized examples:\")\n",
    "    for i in range(1, min(len(y)+1, 4)):\n",
    "        print(f'┌X: \"{X[-i]}\"')\n",
    "        print(f'└y: \"{y[-i]}\"')\n",
    "    print()\n",
    "\n",
    "    if type(X) is list:\n",
    "        X = tuple(X)\n",
    "        y = tuple(y)\n",
    "\n",
    "    return tokenize_inputs(tokenizer, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8Wx14kixomjf",
   "metadata": {
    "id": "8Wx14kixomjf"
   },
   "outputs": [],
   "source": [
    "def batch_detokenize(tokenizer, tokens):\n",
    "    tokens = np.where(tokens != -100, tokens, tokenizer.pad_token_id) # Replace -100 in the labels as we can't decode them\n",
    "    texts = tokenizer.batch_decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "    for i in range(len(texts)):\n",
    "        text = texts[i].strip()\n",
    "        texts[i] = text\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZWryAdwqkzut",
   "metadata": {
    "id": "ZWryAdwqkzut"
   },
   "outputs": [],
   "source": [
    "print(tokenize_and_show_inputs(tokenizer, [\"\"], [\"\"]))\n",
    "print()\n",
    "print(tokenize_and_show_inputs(tokenizer, [\"Holding out for a hero\"], [\"hero\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fb9aae-3aa3-47a4-befe-cf9a80978ebd",
   "metadata": {
    "id": "d1fb9aae-3aa3-47a4-befe-cf9a80978ebd"
   },
   "outputs": [],
   "source": [
    "# Create torch dataset\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx]) # torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "def get_dataset(Xy_tokenized):\n",
    "    dataset = Dataset(Xy_tokenized)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b844adf-e40c-4712-b714-79ece6327c28",
   "metadata": {
    "id": "1b844adf-e40c-4712-b714-79ece6327c28"
   },
   "outputs": [],
   "source": [
    "# DataCollator\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nZulkQLFUh7r",
   "metadata": {
    "id": "nZulkQLFUh7r"
   },
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_dataloader(dataset, batch_size):\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, collate_fn=data_collator\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qc3Ifox5E4_j",
   "metadata": {
    "id": "qc3Ifox5E4_j"
   },
   "source": [
    "# Metrics and Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0vTFTVIgKY5U",
   "metadata": {
    "id": "0vTFTVIgKY5U"
   },
   "outputs": [],
   "source": [
    "X_eval, y_eval = X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kxr8FJsmzEMR",
   "metadata": {
    "id": "kxr8FJsmzEMR"
   },
   "outputs": [],
   "source": [
    "def merge_samples(X, Y):\n",
    "    merged_samples = {}\n",
    "    for x, y in zip(X, Y):\n",
    "        if x not in merged_samples:\n",
    "            merged_samples[x] = set()\n",
    "        merged_samples[x].add(y)\n",
    "    merged_y = [list(merged_samples[x]) for x in X]\n",
    "    return merged_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q2jW6bOw61HO",
   "metadata": {
    "id": "Q2jW6bOw61HO"
   },
   "outputs": [],
   "source": [
    "# Remove duplicate samples after merging their outputs\n",
    "\n",
    "def squeeze_samples(X, candidates, referenceses):\n",
    "    squeezed_candidates_dict = {}\n",
    "    squeezed_referenceses_dict = {}\n",
    "    for i in range(len(X)):\n",
    "        x = X[i]\n",
    "        if x not in squeezed_candidates_dict:\n",
    "            squeezed_candidates_dict[x] = candidates[i]\n",
    "            squeezed_referenceses_dict[x] = referenceses[i]\n",
    "    squeezed_X = list(squeezed_candidates_dict.keys())\n",
    "    squeezed_candidates = list(squeezed_candidates_dict.values())\n",
    "    squeezed_referenceses_dict = list(squeezed_referenceses_dict.values())\n",
    "    return squeezed_X, squeezed_candidates, squeezed_referenceses_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NZRy5cpue9Pu",
   "metadata": {
    "id": "NZRy5cpue9Pu"
   },
   "source": [
    "## Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hWRoXjnE7Hp6",
   "metadata": {
    "id": "hWRoXjnE7Hp6"
   },
   "outputs": [],
   "source": [
    "# \" One | Two | Three \" => [\"One\", \"Two\", \"Three\"]\n",
    "def split_and_clean(text, separator=' '):\n",
    "    return [part_of_text.strip() for part_of_text in text.strip().split(separator)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trzOgwF0HgFa",
   "metadata": {
    "id": "trzOgwF0HgFa"
   },
   "outputs": [],
   "source": [
    "def apply_on_vec_mpqa(vec, t_id=None, p_id=None, i_id=None, t_ids=None, p_ids=None, i_ids=None):\n",
    "    t_index = None\n",
    "\n",
    "    if t_id:\n",
    "        t_ids = [t_id]\n",
    "    if t_ids:\n",
    "        for t_id in t_ids:\n",
    "            if t_id in type_classid2classname:\n",
    "                t_index = type_classid2classindex[t_id]\n",
    "                vec[t_index] = 1\n",
    "\n",
    "    if p_id:\n",
    "        p_ids = [p_id]\n",
    "    if p_ids:\n",
    "        p_startpos = 0\n",
    "        for p_id in p_ids:\n",
    "            if p_id in polarity_classid2classname:\n",
    "                p_index = polarity_classid2classindex[p_id]\n",
    "                vec[p_startpos + p_index] = 1\n",
    "\n",
    "    if i_id:\n",
    "        i_ids = [i_id]\n",
    "    if i_ids:\n",
    "        i_startpos = 0\n",
    "        for i_id in i_ids:\n",
    "            if i_id in intensity_classid2classname:\n",
    "                i_indices = intensity_classid2classindices[i_id]\n",
    "                for i_index in i_indices:\n",
    "                    vec[i_startpos + i_index] = 1\n",
    "\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cGwb4umpA2PW",
   "metadata": {
    "id": "cGwb4umpA2PW"
   },
   "outputs": [],
   "source": [
    "def apply_on_vec_agnews(vec, id=None, ids=None):\n",
    "    if id:\n",
    "        ids = [id]\n",
    "    if ids:\n",
    "        startpos = 0\n",
    "        for id in ids:\n",
    "            if id in agnews_classid2classname:\n",
    "                index = agnews_classid2classindex[id]\n",
    "                vec[startpos + index] = 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DhIovTmM6ypL",
   "metadata": {
    "id": "DhIovTmM6ypL"
   },
   "outputs": [],
   "source": [
    "def apply_on_vec_amzn(vec, id=None, ids=None):\n",
    "    if id:\n",
    "        ids = [id]\n",
    "    if ids:\n",
    "        startpos = 0\n",
    "        for id in ids:\n",
    "            if id in amzn_classid2classname:\n",
    "                index = amzn_classid2classindex[id]\n",
    "                vec[startpos + index] = 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9QvUtA4sNfLH",
   "metadata": {
    "id": "9QvUtA4sNfLH"
   },
   "outputs": [],
   "source": [
    "def outputs_text_to_vec(outputs_text):\n",
    "\n",
    "    if DATASET_NAME == 'MPQA-T':\n",
    "        outputs_vec = np.zeros((len(outputs_text), 4), dtype=int) # Empty 5-dimensional vector\n",
    "        for batch_i in range(len(outputs_text)):\n",
    "            t_ids = split_and_clean(outputs_text[batch_i], ' ')\n",
    "            outputs_vec[batch_i] = apply_on_vec_mpqa(outputs_vec[batch_i], t_ids=t_ids)\n",
    "\n",
    "    elif DATASET_NAME == 'MPQA-P':\n",
    "        outputs_vec = np.zeros((len(outputs_text), 2), dtype=int) # Empty 3-dimensional vector\n",
    "        for batch_i in range(len(outputs_text)):\n",
    "            p_ids = split_and_clean(outputs_text[batch_i], ' ')\n",
    "            outputs_vec[batch_i] = apply_on_vec_mpqa(outputs_vec[batch_i], p_id=p_ids[0])\n",
    "\n",
    "    elif DATASET_NAME == 'MPQA-I':\n",
    "        outputs_vec = np.zeros((len(outputs_text), 3), dtype=int) # Empty 3-dimensional vector\n",
    "        for batch_i in range(len(outputs_text)):\n",
    "            i_ids = split_and_clean(outputs_text[batch_i], ' ')\n",
    "            outputs_vec[batch_i] = apply_on_vec_mpqa(outputs_vec[batch_i], i_ids=i_ids)\n",
    "\n",
    "    elif DATASET_NAME == 'AGNEWS':\n",
    "        outputs_vec = np.zeros((len(outputs_text), 4), dtype=int) # Empty 4-dimensional vector\n",
    "        for batch_i in range(len(outputs_text)):\n",
    "            ids = split_and_clean(outputs_text[batch_i], ' ')\n",
    "            outputs_vec[batch_i] = apply_on_vec_agnews(outputs_vec[batch_i], id=ids[0])\n",
    "\n",
    "    elif DATASET_NAME.startswith('AMZN'):\n",
    "        outputs_vec = np.zeros((len(outputs_text), 5), dtype=int) # Empty 5-dimensional vector\n",
    "        for batch_i in range(len(outputs_text)):\n",
    "            ids = split_and_clean(outputs_text[batch_i], ' ')\n",
    "            outputs_vec[batch_i] = apply_on_vec_amzn(outputs_vec[batch_i], id=ids[0])\n",
    "\n",
    "    return outputs_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tVyDFTibdVBZ",
   "metadata": {
    "id": "tVyDFTibdVBZ"
   },
   "source": [
    "### MPQA Type Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RbYLl01JdkD7",
   "metadata": {
    "id": "RbYLl01JdkD7"
   },
   "outputs": [],
   "source": [
    "def calculate_type_metrics(preds, targets, decimals=6):\n",
    "    targets = targets[:, :NUM_TYPE_CLASSES]\n",
    "    preds   = preds[:, :NUM_TYPE_CLASSES]\n",
    "\n",
    "    accuracy_list = np.sum(preds == targets, axis=0).astype(float) / preds.shape[0]\n",
    "    f1_list = f1_score(y_true=targets, y_pred=preds, zero_division=0, average=None)\n",
    "    precision_list = precision_score(y_true=targets, y_pred=preds, zero_division=0, average=None)\n",
    "    recall_list = recall_score(y_true=targets, y_pred=preds, zero_division=0, average=None)\n",
    "\n",
    "    TP = np.sum(preds & targets)\n",
    "    FP = np.sum(preds & (1-targets))\n",
    "    FN = np.sum((1-preds) & targets)\n",
    "\n",
    "    if TP + FP > 0:\n",
    "        micro_precision = TP / (TP + FP)\n",
    "    else:\n",
    "        micro_precision = 1\n",
    "\n",
    "    if TP + FN > 0:\n",
    "        micro_recall = TP / (TP + FN)\n",
    "    else:\n",
    "        micro_recall = 1\n",
    "\n",
    "    micro_f1 = statistics.harmonic_mean([micro_precision, micro_recall])\n",
    "    micro_accuracy = np.mean(preds == targets)\n",
    "\n",
    "    exact_match_ratio = np.mean(np.sum(preds == targets, axis = 1) == NUM_TYPE_CLASSES)\n",
    "\n",
    "    return {\n",
    "        'type_exact_match_ratio': np.round(exact_match_ratio, decimals),\n",
    "        'type_micro_f1': np.round(micro_f1, decimals),\n",
    "        'type_micro_precision': np.round(micro_precision, decimals),\n",
    "        'type_micro_recall': np.round(micro_recall, decimals),\n",
    "        'type_micro_accuracy': np.round(np.mean(micro_accuracy), decimals),\n",
    "        'type_f1': np.round(f1_list, decimals).tolist(),\n",
    "        'type_precision': np.round(precision_list, decimals).tolist(),\n",
    "        'type_recall': np.round(recall_list, decimals).tolist(),\n",
    "        'type_accuracy': np.round(accuracy_list, decimals).tolist(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pSgNzr7geHaa",
   "metadata": {
    "id": "pSgNzr7geHaa"
   },
   "source": [
    "### MPQA Polarity Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ebfftf57VPh0",
   "metadata": {
    "id": "Ebfftf57VPh0"
   },
   "outputs": [],
   "source": [
    "def calculate_polarity_f1_measure(extracted_polarity_preds, extracted_polarity_targets, average='weighted'):\n",
    "    extracted_polarity_targets = np.argmax(extracted_polarity_targets, axis=-1)\n",
    "    extracted_polarity_preds   = np.argmax(extracted_polarity_preds,   axis=-1)\n",
    "    polarity_f1 = f1_score(extracted_polarity_targets, extracted_polarity_preds, average=average)\n",
    "    return polarity_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7F1SuOxLaYRY",
   "metadata": {
    "id": "7F1SuOxLaYRY"
   },
   "outputs": [],
   "source": [
    "def calculate_polarity_correctness_score(extracted_polarity_preds, extracted_polarity_targets):\n",
    "    extracted_polarity_targets = np.argmax(extracted_polarity_targets, axis=-1)\n",
    "    extracted_polarity_preds   = np.argmax(extracted_polarity_preds,   axis=-1)\n",
    "    polarity_score = np.sum(extracted_polarity_preds == extracted_polarity_targets)\n",
    "    return polarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dzNgqFOraaXS",
   "metadata": {
    "id": "dzNgqFOraaXS"
   },
   "outputs": [],
   "source": [
    "def calculate_polarity_metrics(preds, targets, decimals=6):\n",
    "    batch_size = len(preds)\n",
    "\n",
    "    extracted_polarity_targets = targets\n",
    "    extracted_polarity_preds   = preds\n",
    "\n",
    "    number_of_samples = batch_size\n",
    "\n",
    "    polarity_acc = calculate_polarity_correctness_score(extracted_polarity_preds, extracted_polarity_targets) / number_of_samples\n",
    "    polarity_weighted_f1 = calculate_polarity_f1_measure(extracted_polarity_preds, extracted_polarity_targets, average='weighted')\n",
    "    polarity_micro_f1 = calculate_polarity_f1_measure(extracted_polarity_preds, extracted_polarity_targets, average='micro')\n",
    "\n",
    "    return {\n",
    "        'polarity_accuracy': np.round(polarity_acc, decimals),\n",
    "        'polarity_weighted_f1': np.round(polarity_weighted_f1, decimals),\n",
    "        'polarity_micro_f1': np.round(polarity_micro_f1, decimals),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ejh-pD5ceUdB",
   "metadata": {
    "id": "Ejh-pD5ceUdB"
   },
   "source": [
    "### MPQA Intensity Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PY58rZzsKuvN",
   "metadata": {
    "id": "PY58rZzsKuvN"
   },
   "outputs": [],
   "source": [
    "# Convert 3-dim intensity vector to its corresponding number\n",
    "\n",
    "def get_intensity_id(vec):\n",
    "    vec = vec.tolist()\n",
    "    if vec == [1, 0, 0]:\n",
    "        return 0\n",
    "    if vec == [1, 1, 0]:\n",
    "        return 1\n",
    "    if vec == [0, 1, 0]:\n",
    "        return 2\n",
    "    if vec == [0, 1, 1]:\n",
    "        return 3\n",
    "    if vec == [0, 0, 1]:\n",
    "        return 4\n",
    "    return 1 # everything is equal to low-medium by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KfWRGMa3IRto",
   "metadata": {
    "id": "KfWRGMa3IRto"
   },
   "outputs": [],
   "source": [
    "def calculate_intensity_d_correctness_score(extracted_intensity_preds, extracted_intensity_targets, d=1):\n",
    "    batch_size = len(extracted_intensity_preds)\n",
    "    intensity_score = 0\n",
    "    for i in range(batch_size):\n",
    "        target_id = get_intensity_id(extracted_intensity_targets[i]) # everything is equal to low-medium by default\n",
    "        pred_id   = get_intensity_id(extracted_intensity_preds[i])   # everything is equal to low-medium by default\n",
    "        if abs(target_id - pred_id) <= d:\n",
    "            intensity_score += 1\n",
    "    return intensity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TmjDLJJjU-nJ",
   "metadata": {
    "id": "TmjDLJJjU-nJ"
   },
   "outputs": [],
   "source": [
    "def calculate_intensity_custom_weighted_f1_measure(extracted_intensity_preds, extracted_intensity_targets):\n",
    "\n",
    "    whole = 1.0\n",
    "    with_penalty = 0\n",
    "    half_false = 1\n",
    "    thresh = 0.5\n",
    "    trues = {'medium': 0, 'medium-high': 0, 'low': 0, 'high': 0, 'low-medium': 0}  # TP\n",
    "    cnt = {'medium': 0, 'medium-high': 0, 'low': 0, 'high': 0, 'low-medium': 0}    # TP + FN\n",
    "    falses = {'medium': 0, 'medium-high': 0, 'low': 0, 'high': 0, 'low-medium': 0} # FP\n",
    "\n",
    "    preds_indices = np.argmax(extracted_intensity_preds, axis=-1)\n",
    "\n",
    "    for i in range(len(preds_indices)):\n",
    "\n",
    "        if preds_indices[i] == 0:\n",
    "            if extracted_intensity_targets[i][0] == 0 and  extracted_intensity_targets[i][1] == 0 and  extracted_intensity_targets[i][2] == 1:\n",
    "                falses['low'] += 1\n",
    "            if extracted_intensity_targets[i][0] == 0 and  extracted_intensity_targets[i][1] == 1 and  extracted_intensity_targets[i][2] == 0:\n",
    "                falses['low'] += half_false\n",
    "            if extracted_intensity_targets[i][0] == 0 and  extracted_intensity_targets[i][1] == 1 and  extracted_intensity_targets[i][2] == 1:\n",
    "                falses['low'] += half_false\n",
    "\n",
    "        if preds_indices[i] == 1:\n",
    "            if extracted_intensity_targets[i][0] == 0 and  extracted_intensity_targets[i][1] == 0 and  extracted_intensity_targets[i][2] == 1:\n",
    "                falses['medium'] += half_false\n",
    "            if extracted_intensity_targets[i][0] == 1 and  extracted_intensity_targets[i][1] == 0 and  extracted_intensity_targets[i][2] == 0:\n",
    "                falses['medium'] += half_false\n",
    "\n",
    "        if preds_indices[i] == 2:\n",
    "            if extracted_intensity_targets[i][0] == 1 and  extracted_intensity_targets[i][1] == 0 and  extracted_intensity_targets[i][2] == 0:\n",
    "                falses['high'] += 1\n",
    "            if extracted_intensity_targets[i][0] == 1 and  extracted_intensity_targets[i][1] == 1 and  extracted_intensity_targets[i][2] == 0:\n",
    "                falses['high'] += 1\n",
    "            if extracted_intensity_targets[i][0] == 0 and  extracted_intensity_targets[i][1] == 1 and  extracted_intensity_targets[i][2] == 1:\n",
    "                falses['high'] += half_false\n",
    "            if extracted_intensity_targets[i][0] == 0 and  extracted_intensity_targets[i][1] == 1 and  extracted_intensity_targets[i][2] == 0:\n",
    "                falses['high'] += half_false\n",
    "\n",
    "        if extracted_intensity_targets[i][0] == 1 and  extracted_intensity_targets[i][1] == 0 and  extracted_intensity_targets[i][2] == 0:\n",
    "            cnt['low'] += 1\n",
    "            if preds_indices[i] == 0:\n",
    "                trues['low'] += whole\n",
    "\n",
    "        elif extracted_intensity_targets[i][0] == 0 and  extracted_intensity_targets[i][1] == 1 and  extracted_intensity_targets[i][2] == 0:\n",
    "            cnt['medium'] += 1\n",
    "            if preds_indices[i] == 1:\n",
    "                trues['medium'] += whole\n",
    "            else:\n",
    "                trues['medium'] += with_penalty\n",
    "\n",
    "        elif extracted_intensity_targets[i][0] == 0 and  extracted_intensity_targets[i][1] == 0 and  extracted_intensity_targets[i][2] == 1:\n",
    "            cnt['high'] += 1\n",
    "            if preds_indices[i] == 2:\n",
    "                trues['high'] += whole\n",
    "\n",
    "        elif extracted_intensity_targets[i][0] == 1 and  extracted_intensity_targets[i][1] == 1 and  extracted_intensity_targets[i][2] == 0:\n",
    "            cnt['low-medium'] += 1\n",
    "            if preds_indices[i] == 0 or preds_indices[i] == 1:\n",
    "                trues['low-medium'] += whole\n",
    "\n",
    "        elif extracted_intensity_targets[i][0] == 0 and  extracted_intensity_targets[i][1] == 1 and  extracted_intensity_targets[i][2] == 1:\n",
    "            cnt['medium-high'] += 1\n",
    "            if preds_indices[i] == 1 or preds_indices[i] == 2:\n",
    "                trues['medium-high'] += whole\n",
    "\n",
    "    weighted_f1 = 0\n",
    "    weights = 0\n",
    "    for intensity_class in trues.keys():\n",
    "        try:\n",
    "            intensity_class_precision = trues[intensity_class] / (trues[intensity_class] + falses[intensity_class])\n",
    "        except:\n",
    "            intensity_class_precision = 1\n",
    "        try:\n",
    "            intensity_class_recall = trues[intensity_class] / cnt[intensity_class]\n",
    "        except:\n",
    "            intensity_class_recall = 1\n",
    "        intensity_class_f1 = statistics.harmonic_mean([intensity_class_precision, intensity_class_recall])\n",
    "        weighted_f1 += intensity_class_f1 * cnt[intensity_class]\n",
    "        weights += cnt[intensity_class]\n",
    "    custom_intensity_weighted_f1 = weighted_f1 / weights\n",
    "\n",
    "    return custom_intensity_weighted_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bl-rW6LfacIX",
   "metadata": {
    "id": "bl-rW6LfacIX"
   },
   "outputs": [],
   "source": [
    "def calculate_intensity_custom_correctness_score(extracted_intensity_preds, extracted_intensity_targets):\n",
    "    extracted_intensity_preds_argmax = np.argmax(extracted_intensity_preds, axis=-1)\n",
    "    intensity_custom_score = 0\n",
    "    for i in range(len(extracted_intensity_preds_argmax)):\n",
    "        if (extracted_intensity_targets[i, extracted_intensity_preds_argmax[i]] == 1):\n",
    "            intensity_custom_score += 1\n",
    "    return intensity_custom_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pmzE32JJafGz",
   "metadata": {
    "id": "pmzE32JJafGz"
   },
   "outputs": [],
   "source": [
    "def calculate_intensity_metrics(preds, targets, decimals=6):\n",
    "    batch_size = len(preds)\n",
    "\n",
    "    extracted_intensity_targets = targets\n",
    "    extracted_intensity_preds = preds\n",
    "\n",
    "    number_of_samples = batch_size\n",
    "\n",
    "    intensity_acc    = calculate_intensity_d_correctness_score(extracted_intensity_preds, extracted_intensity_targets, d=0) / number_of_samples\n",
    "    intensity_d1_acc = calculate_intensity_d_correctness_score(extracted_intensity_preds, extracted_intensity_targets, d=1) / number_of_samples\n",
    "    intensity_d2_acc = calculate_intensity_d_correctness_score(extracted_intensity_preds, extracted_intensity_targets, d=2) / number_of_samples\n",
    "    intensity_d3_acc = calculate_intensity_d_correctness_score(extracted_intensity_preds, extracted_intensity_targets, d=3) / number_of_samples\n",
    "    intensity_d4_acc = calculate_intensity_d_correctness_score(extracted_intensity_preds, extracted_intensity_targets, d=4) / number_of_samples\n",
    "    intensity_custom_acc = calculate_intensity_custom_correctness_score(extracted_intensity_preds, extracted_intensity_targets) / number_of_samples\n",
    "    intensity_custom_weighted_f1 = calculate_intensity_custom_weighted_f1_measure(extracted_intensity_preds, extracted_intensity_targets)\n",
    "\n",
    "    return {\n",
    "        'intensity_accuracy': np.round(intensity_acc, decimals),\n",
    "        'intensity_d1_accuracy': np.round(intensity_d1_acc, decimals),\n",
    "        'intensity_d2_accuracy': np.round(intensity_d2_acc, decimals),\n",
    "        'intensity_d3_accuracy': np.round(intensity_d3_acc, decimals),\n",
    "        'intensity_d4_accuracy': np.round(intensity_d4_acc, decimals),\n",
    "        'intensity_custom_accuracy': np.round(intensity_custom_acc, decimals),\n",
    "        'intensity_custom_weighted_f1': np.round(intensity_custom_weighted_f1, decimals),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eCbgcZVoCk1P",
   "metadata": {
    "id": "eCbgcZVoCk1P"
   },
   "source": [
    "### AG News Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jp7jqXAaCk1P",
   "metadata": {
    "id": "jp7jqXAaCk1P"
   },
   "outputs": [],
   "source": [
    "def calculate_agnews_f1_measure(extracted_preds, extracted_targets, average='weighted'):\n",
    "    extracted_targets = np.argmax(extracted_targets, axis=-1)\n",
    "    extracted_preds   = np.argmax(extracted_preds,   axis=-1)\n",
    "    weighted_f1 = f1_score(extracted_targets, extracted_preds, average=average)\n",
    "    return weighted_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-9irBri8Ck1Q",
   "metadata": {
    "id": "-9irBri8Ck1Q"
   },
   "outputs": [],
   "source": [
    "def calculate_agnews_correctness_score(extracted_preds, extracted_targets):\n",
    "    extracted_targets = np.argmax(extracted_targets, axis=-1)\n",
    "    extracted_preds   = np.argmax(extracted_preds,   axis=-1)\n",
    "    score = np.sum(extracted_preds == extracted_targets)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HDidr9kuCk1Q",
   "metadata": {
    "id": "HDidr9kuCk1Q"
   },
   "outputs": [],
   "source": [
    "def calculate_agnews_metrics(preds, targets, decimals=6):\n",
    "    batch_size = len(preds)\n",
    "\n",
    "    extracted_targets = targets\n",
    "    extracted_preds   = preds\n",
    "\n",
    "    number_of_samples = batch_size\n",
    "\n",
    "    acc = calculate_agnews_correctness_score(extracted_preds, extracted_targets) / number_of_samples\n",
    "    weighted_f1 = calculate_agnews_f1_measure(extracted_preds, extracted_targets, average='weighted')\n",
    "    macro_f1 = calculate_agnews_f1_measure(extracted_preds, extracted_targets, average='macro')\n",
    "\n",
    "    return {\n",
    "        'agnews_accuracy': np.round(acc, decimals),\n",
    "        'agnews_weighted_f1': np.round(weighted_f1, decimals),\n",
    "        'agnews_macro_f1': np.round(macro_f1, decimals),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7HkhOZmF7o74",
   "metadata": {
    "id": "7HkhOZmF7o74"
   },
   "source": [
    "### Amazon Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BsLkA9iP7o74",
   "metadata": {
    "id": "BsLkA9iP7o74"
   },
   "outputs": [],
   "source": [
    "def calculate_amzn_f1_measure(extracted_preds, extracted_targets, average='weighted'):\n",
    "    extracted_targets = np.argmax(extracted_targets, axis=-1)\n",
    "    extracted_preds   = np.argmax(extracted_preds,   axis=-1)\n",
    "    weighted_f1 = f1_score(extracted_targets, extracted_preds, average=average)\n",
    "    return weighted_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ZmdrU7A7o75",
   "metadata": {
    "id": "5ZmdrU7A7o75"
   },
   "outputs": [],
   "source": [
    "def calculate_amzn_correctness_score(extracted_preds, extracted_targets):\n",
    "    extracted_targets = np.argmax(extracted_targets, axis=-1)\n",
    "    extracted_preds   = np.argmax(extracted_preds,   axis=-1)\n",
    "    score = np.sum(extracted_preds == extracted_targets)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kQv7Tc5I7o75",
   "metadata": {
    "id": "kQv7Tc5I7o75"
   },
   "outputs": [],
   "source": [
    "def calculate_amzn_metrics(preds, targets, decimals=6):\n",
    "    batch_size = len(preds)\n",
    "\n",
    "    extracted_targets = targets\n",
    "    extracted_preds   = preds\n",
    "\n",
    "    number_of_samples = batch_size\n",
    "\n",
    "    acc = calculate_amzn_correctness_score(extracted_preds, extracted_targets) / number_of_samples\n",
    "    weighted_f1 = calculate_amzn_f1_measure(extracted_preds, extracted_targets, average='weighted')\n",
    "    macro_f1 = calculate_amzn_f1_measure(extracted_preds, extracted_targets, average='macro')\n",
    "\n",
    "    return {\n",
    "        'amzn_accuracy': np.round(acc, decimals),\n",
    "        'amzn_weighted_f1': np.round(weighted_f1, decimals),\n",
    "        'amzn_macro_f1': np.round(macro_f1, decimals),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LowrJ4U9dO6q",
   "metadata": {
    "id": "LowrJ4U9dO6q"
   },
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Romelqk1ajX-",
   "metadata": {
    "id": "Romelqk1ajX-"
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(pred):\n",
    "    preds_array = pred.predictions[0] if isinstance(pred.predictions, tuple) else pred.predictions\n",
    "    is_classification = (preds_array.dtype in [np.float32, np.float64, float])\n",
    "    \n",
    "    if is_classification:\n",
    "        preds_indices = np.argmax(preds_array, axis=-1)\n",
    "        \n",
    "        targets_indices = np.array(pred.label_ids)\n",
    "        \n",
    "        if len(targets_indices.shape) == 2:\n",
    "            if targets_indices.shape[1] == len(CLASS_IDS):\n",
    "                targets_indices = np.argmax(targets_indices, axis=1)\n",
    "            else:\n",
    "                mapped_targets = []\n",
    "                for row in targets_indices:\n",
    "                    found_idx = 0\n",
    "                    for idx, tok in enumerate(CLASSES_TOKENS):\n",
    "                        if tok in row:\n",
    "                            found_idx = idx\n",
    "                            break\n",
    "                    mapped_targets.append(found_idx)\n",
    "                targets_indices = np.array(mapped_targets)\n",
    "    \n",
    "        preds = [CLASS_IDS[int(idx)] for idx in preds_indices]\n",
    "        targets = [CLASS_IDS[int(idx)] for idx in targets_indices]\n",
    "        \n",
    "    else:\n",
    "        targets = np.array(pred.label_ids, dtype=int)\n",
    "        targets = batch_detokenize(tokenizer, targets)\n",
    "\n",
    "        preds = pred.predictions\n",
    "        preds = batch_detokenize(tokenizer, preds)\n",
    "\n",
    "    candidates = preds\n",
    "    referenceses = merge_samples(X_eval, targets)\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    if DATASET_NAME.startswith('MPQA'):\n",
    "        targets = outputs_text_to_vec(targets)\n",
    "        preds   = outputs_text_to_vec(preds)\n",
    "\n",
    "        if DATASET_NAME == 'MPQA-T':\n",
    "            metrics.update(calculate_type_metrics(preds, targets))\n",
    "            metrics['main_metric'] = metrics['type_micro_f1']\n",
    "\n",
    "        elif DATASET_NAME == 'MPQA-P':\n",
    "            metrics.update(calculate_polarity_metrics(preds, targets))\n",
    "            metrics['main_metric'] = metrics['polarity_accuracy']\n",
    "\n",
    "        elif DATASET_NAME == 'MPQA-I':\n",
    "            metrics.update(calculate_intensity_metrics(preds, targets))\n",
    "            metrics['main_metric'] = metrics['intensity_accuracy']\n",
    "\n",
    "    elif DATASET_NAME == 'AGNEWS':\n",
    "        targets = outputs_text_to_vec(targets)\n",
    "        preds   = outputs_text_to_vec(preds)\n",
    "        metrics.update(calculate_agnews_metrics(preds, targets))\n",
    "        metrics['main_metric'] = metrics['agnews_accuracy']\n",
    "\n",
    "    elif DATASET_NAME.startswith('AMZN'):\n",
    "        targets = outputs_text_to_vec(targets)\n",
    "        preds   = outputs_text_to_vec(preds)\n",
    "        metrics.update(calculate_amzn_metrics(preds, targets))\n",
    "        metrics['main_metric'] = metrics['amzn_accuracy']\n",
    "\n",
    "    if X_eval == X_val:\n",
    "        print('┌X:          ', X_eval[0])\n",
    "        print('├Candidate:  ', candidates[0])\n",
    "        print('└References: ', referenceses[0])\n",
    "        print('┌X:          ', X_eval[-1])\n",
    "        print('├Candidate:  ', candidates[-1])\n",
    "        print('└References: ', referenceses[-1])\n",
    "        print('--- Next Epoch ---')\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfde22e-6ec5-409d-9d5f-bafb21d8d0e9",
   "metadata": {
    "id": "2dfde22e-6ec5-409d-9d5f-bafb21d8d0e9"
   },
   "source": [
    "# Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QdusgCNayB0B",
   "metadata": {
    "id": "QdusgCNayB0B"
   },
   "outputs": [],
   "source": [
    "# Training Arguments\n",
    "\n",
    "def get_training_args(fold_counter, seed_counter):\n",
    "    # USE CustomTrainingArguments instead of Seq2SeqTrainingArguments\n",
    "    training_args = CustomTrainingArguments(\n",
    "        output_dir = f'models/{EXPERIMENT_NAME}_{fold_counter}_{seed_counter}',\n",
    "        overwrite_output_dir = True,\n",
    "        per_device_train_batch_size = PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "        per_device_eval_batch_size = PER_DEVICE_VAL_BATCH_SIZE,\n",
    "        local_rank = LOCAL_RANK,\n",
    "        fp16 = FP16,\n",
    "        fp16_opt_level = FP16_OPT_LEVEL,\n",
    "        fp16_full_eval = FP16_FULL_EVAL,\n",
    "        logging_strategy = LOGGING_STRATEGY,\n",
    "        evaluation_strategy = EVAL_STRATEGY,\n",
    "        save_strategy = SAVE_STRATEGY,\n",
    "        save_total_limit = save_total_limit,\n",
    "        num_train_epochs = NUM_TRAIN_EPOCHS,\n",
    "        load_best_model_at_end = LOAD_BEST_MODEL_AT_END,\n",
    "        metric_for_best_model = METRIC_FOR_BEST_MODEL,\n",
    "        dataloader_num_workers = NUM_WORKERS,\n",
    "        seed = SEED,\n",
    "        group_by_length = True,\n",
    "        predict_with_generate = PREDICT_WITH_GENERATE,\n",
    "        report_to = \"none\",\n",
    "        full_determinism = True,\n",
    "        \n",
    "        # Pass new parameters to arguments object\n",
    "        use_contrastive_loss = USE_CONTRASTIVE_LOSS,\n",
    "        contrastive_loss_alpha = CONTRASTIVE_LOSS_ALPHA,\n",
    "        similarity_metric = SIMILARITY_METRIC,\n",
    "        temperature = TEMPERATURE,\n",
    "        use_hard_negative_mining = USE_HARD_NEGATIVE_MINING,\n",
    "        hard_negative_mining_m = HARD_NEGATIVE_MINING_M\n",
    "    )\n",
    "    print(\"Number of GPUs:\", training_args.n_gpu)\n",
    "    print(\"Parallel Mode:\", training_args.parallel_mode)\n",
    "    print()\n",
    "    return training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Mrgr2ccAX40c",
   "metadata": {
    "id": "Mrgr2ccAX40c"
   },
   "outputs": [],
   "source": [
    "def free_gpu():\n",
    "    try:\n",
    "        model\n",
    "    except NameError:\n",
    "        None\n",
    "    else:\n",
    "        model.cpu()\n",
    "        del model\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bc37eb-7517-4775-a7de-37954e683c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomTrainingArguments(Seq2SeqTrainingArguments):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.use_contrastive_loss = kwargs.pop('use_contrastive_loss', False)\n",
    "        self.contrastive_loss_alpha = kwargs.pop('contrastive_loss_alpha', 0.1)\n",
    "        self.similarity_metric = kwargs.pop('similarity_metric', 'cosine')\n",
    "        self.temperature = kwargs.pop('temperature', 0.07)\n",
    "        self.use_hard_negative_mining = kwargs.pop('use_hard_negative_mining', False)\n",
    "        self.hard_negative_mining_m = kwargs.pop('hard_negative_mining_m', 5)\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "class ContrastiveLearningTrainer(Seq2SeqTrainer):\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        if not model.config.is_encoder_decoder:\n",
    "            return Trainer.prediction_step(self, model, inputs, prediction_loss_only, ignore_keys)\n",
    "        return super().prediction_step(model, inputs, prediction_loss_only, ignore_keys)\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        if not model.config.is_encoder_decoder:\n",
    "            if len(labels.shape) == 2:\n",
    "                if labels.shape[1] == model.config.num_labels:\n",
    "                    labels = torch.argmax(labels, dim=1)\n",
    "                else:\n",
    "                    labels_1d = []\n",
    "                    for row in labels:\n",
    "                        found_idx = 0\n",
    "                        for idx, tok in enumerate(CLASSES_TOKENS):\n",
    "                            if (row == tok).any():\n",
    "                                found_idx = idx\n",
    "                                break\n",
    "                        labels_1d.append(found_idx)\n",
    "                    labels = torch.tensor(labels_1d, device=labels.device).long()\n",
    "            labels = labels.long()\n",
    "            \n",
    "        inputs[\"labels\"] = labels\n",
    "        inputs[\"output_hidden_states\"] = True\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        ce_loss = outputs.loss\n",
    "\n",
    "        if not self.args.use_contrastive_loss:\n",
    "            return (ce_loss, outputs) if return_outputs else ce_loss\n",
    "\n",
    "        if hasattr(outputs, 'encoder_last_hidden_state'):\n",
    "            last_hidden_state = outputs.encoder_last_hidden_state\n",
    "        elif hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:\n",
    "            last_hidden_state = outputs.hidden_states[-1]\n",
    "        else:\n",
    "            inputs_for_base = {k: v for k, v in inputs.items() if k not in [\"labels\", \"output_hidden_states\"]}\n",
    "            base_model_output = getattr(model, model.base_model_prefix)(**inputs_for_base)\n",
    "            last_hidden_state = base_model_output.last_hidden_state\n",
    "\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        embeddings = last_hidden_state[:, 0, :]\n",
    "\n",
    "        class_labels = labels[:, 0] if labels.dim() > 1 else labels\n",
    "        scl_loss = self.calculate_scl(embeddings, class_labels)\n",
    "        \n",
    "        total_loss = ce_loss + self.args.contrastive_loss_alpha * scl_loss\n",
    "        return (total_loss, outputs) if return_outputs else total_loss\n",
    "    \n",
    "    def calculate_scl(self, embeddings, labels):\n",
    "        batch_size = embeddings.shape[0]\n",
    "        labels = labels.view(-1, 1)\n",
    "        mask = torch.eq(labels, labels.T).float().to(self.args.device)\n",
    "        \n",
    "        if self.args.similarity_metric == 'cosine':\n",
    "            embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "            sim_matrix = torch.matmul(embeddings, embeddings.T)\n",
    "        else:\n",
    "            sim_matrix = -torch.cdist(embeddings, embeddings, p=2)\n",
    "            \n",
    "        sim_matrix = sim_matrix / self.args.temperature\n",
    "        \n",
    "        logits_mask = torch.scatter(torch.ones_like(mask), 1, torch.arange(batch_size).view(-1, 1).to(self.args.device), 0)\n",
    "        mask = mask * logits_mask\n",
    "        \n",
    "        exp_logits = torch.exp(sim_matrix) * logits_mask\n",
    "        \n",
    "        if self.args.use_hard_negative_mining:\n",
    "            M = self.args.hard_negative_mining_m\n",
    "            neg_mask = (1 - mask) * logits_mask\n",
    "            \n",
    "            neg_sim = sim_matrix.masked_fill(neg_mask == 0, -float('inf'))\n",
    "            \n",
    "            actual_m = min(M, neg_sim.shape[1])\n",
    "            \n",
    "            if actual_m > 0:\n",
    "                _, top_indices = torch.topk(neg_sim, k=actual_m, dim=1)\n",
    "                \n",
    "                hard_neg_mask = torch.zeros_like(neg_mask).scatter_(1, top_indices, 1.0)\n",
    "\n",
    "                hard_neg_mask = hard_neg_mask * neg_mask\n",
    "                \n",
    "                valid_denominator_mask = mask + hard_neg_mask\n",
    "                \n",
    "                exp_logits = exp_logits * valid_denominator_mask\n",
    "\n",
    "        log_prob = sim_matrix - torch.log(exp_logits.sum(1, keepdim=True) + 1e-9)\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-9)\n",
    "        \n",
    "        loss = -mean_log_prob_pos\n",
    "        loss = loss[~torch.isnan(loss)]\n",
    "        \n",
    "        return loss.mean() if loss.numel() > 0 else torch.tensor(0.0, device=self.args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146c6fa2-612e-4fcb-ac38-a6bbf9a53624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_alignment_and_uniformity(embeddings, labels):\n",
    "    \"\"\"\n",
    "    Calculates the alignment and uniformity metrics for the given embeddings.\n",
    "    \n",
    "    Args:\n",
    "        embeddings (torch.Tensor or np.ndarray): The input embeddings.\n",
    "        labels (torch.Tensor or np.ndarray): The corresponding labels for the embeddings.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing the 'alignment' and 'uniformity' scores.\n",
    "    \"\"\"\n",
    "    if isinstance(embeddings, np.ndarray):\n",
    "        embeddings = torch.from_numpy(embeddings).to(device)\n",
    "    if isinstance(labels, np.ndarray):\n",
    "        labels = torch.from_numpy(labels).to(device)\n",
    "\n",
    "    # Ensure embeddings and labels are on the correct device\n",
    "    embeddings = embeddings.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    # L2 normalization, which is crucial for uniformity calculation and cosine-based alignment\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    \n",
    "    # --- 1. Calculate Alignment ---\n",
    "    # Expected squared Euclidean distance between positive pairs (samples of the same class)\n",
    "    def alignment(x, y):\n",
    "        return (x - y).norm(p=2, dim=1).pow(2).mean()\n",
    "\n",
    "    # --- 2. Calculate Uniformity ---\n",
    "    # Measures the uniform distribution of embeddings on the hypersphere\n",
    "    def uniformity(x):\n",
    "        return torch.pdist(x, p=2).pow(2).mul(-2).exp().mean().log()\n",
    "\n",
    "    # Calculate alignment grouped by class\n",
    "    unique_labels = torch.unique(labels)\n",
    "    align_scores = []\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        # Get all embeddings for the current class\n",
    "        class_embeddings = embeddings[labels == label]\n",
    "        \n",
    "        if len(class_embeddings) > 1:\n",
    "            # Randomly sample two distinct sets of embeddings to compute alignment\n",
    "            indices1 = torch.randperm(len(class_embeddings))\n",
    "            indices2 = torch.randperm(len(class_embeddings))\n",
    "            \n",
    "            # Ensure indices1 and indices2 do not overlap to simulate (x, x+) pairs\n",
    "            while torch.any(indices1 == indices2):\n",
    "                indices2 = torch.randperm(len(class_embeddings))\n",
    "\n",
    "            align_scores.append(alignment(class_embeddings[indices1], class_embeddings[indices2]))\n",
    "\n",
    "    # Compute the mean of the alignment scores across all classes\n",
    "    final_alignment_score = torch.tensor(align_scores).mean() if align_scores else torch.tensor(float('nan'))\n",
    "    final_uniformity_score = uniformity(embeddings)\n",
    "\n",
    "    return {\n",
    "        'alignment': final_alignment_score.item(),\n",
    "        'uniformity': final_uniformity_score.item()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8f5ba7-7694-456d-b14a-1432281d3041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def plot_final_tsne(trainer, X_test_data, y_test_data, total_samples_k, experiment_title):\n",
    "    # Check if the current state is the final state where K=100\n",
    "    if total_samples_k != 100:\n",
    "        print(f\"Current sample size is K={total_samples_k}. Skipping t-SNE visualization.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"K={total_samples_k} reached. Generating t-SNE visualization for dataset {DATASET_NAME}...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. Retrieve the final model from the trainer and extract test set embeddings\n",
    "    print(\"Extracting test set embeddings...\")\n",
    "    model_for_viz = trainer.model\n",
    "    model_for_viz.eval()  # Switch to evaluation mode\n",
    "\n",
    "    # Select the appropriate embedding extraction method based on the model architecture\n",
    "    embedding_method = 'FLAN-T5-ENCODER' if 'flan-t5' in MODEL_NAME.lower() else 'BART-ENCODER'\n",
    "    \n",
    "    test_embeddings = get_embedding(\n",
    "        X=X_test_data, \n",
    "        y=y_test_data, \n",
    "        embedding_method=embedding_method, \n",
    "        model=model_for_viz\n",
    "    )\n",
    "\n",
    "    # 2. Convert text labels to numerical values for plotting and retrieve class names\n",
    "    print(\"Processing label mappings...\")\n",
    "    \n",
    "    if DATASET_NAME == 'MPQA-T':\n",
    "\n",
    "        y_test_data_for_viz = [label.split(' ')[0] for label in y_test_data]\n",
    "        labels_to_ids = type_classid2classindex\n",
    "        class_names = list(type_classname2classindex.keys())\n",
    "        numeric_labels = [labels_to_ids[label] for label in y_test_data_for_viz]\n",
    "\n",
    "    elif DATASET_NAME == 'MPQA-P':\n",
    "        labels_to_ids = polarity_classid2classindex\n",
    "        class_names = list(polarity_classname2classindex.keys())\n",
    "        numeric_labels = [labels_to_ids[label] for label in y_test_data]\n",
    "        \n",
    "    elif DATASET_NAME == 'MPQA-I':\n",
    "        # For MPQA-I, the labels are more complex. We map them to integers from 0 to 4.\n",
    "        labels_to_ids = {\n",
    "            'low': 0, \n",
    "            'low medium': 1, \n",
    "            'medium': 2, \n",
    "            'medium high': 3, \n",
    "            'high': 4\n",
    "        }\n",
    "        class_names = list(labels_to_ids.keys())\n",
    "        # Convert class_id from the notebook (e.g., 'slight') to class_name ('low')\n",
    "        id_to_name_map = intensity_classid2classname\n",
    "        numeric_labels = [labels_to_ids[id_to_name_map[label]] for label in y_test_data]\n",
    "        \n",
    "    elif DATASET_NAME == 'AGNEWS':\n",
    "        labels_to_ids = agnews_classid2classindex\n",
    "        class_names = ['World', 'Sports', 'Business', 'Sci/Tech']  # Use more readable names\n",
    "        numeric_labels = [labels_to_ids[label] for label in y_test_data]\n",
    "        \n",
    "    elif DATASET_NAME.startswith('AMZN'):\n",
    "        labels_to_ids = amzn_classid2classindex\n",
    "        class_names = [f'{i} Star' for i in amzn_classname2classindex.keys()]\n",
    "        numeric_labels = [labels_to_ids[label] for label in y_test_data]\n",
    "        \n",
    "    else:\n",
    "        # Provide a generic fallback for unknown datasets\n",
    "        print(f\"Specific label mapping not found for {DATASET_NAME}. Using generic mapping.\")\n",
    "        unique_labels = sorted(list(set(y_test_data)))\n",
    "        labels_to_ids = {label: i for i, label in enumerate(unique_labels)}\n",
    "        class_names = [str(name) for name in labels_to_ids.keys()]\n",
    "        numeric_labels = [labels_to_ids[label] for label in y_test_data]\n",
    "\n",
    "    print(\"Calculating alignment and uniformity metrics...\")\n",
    "    metrics = calculate_alignment_and_uniformity(test_embeddings, np.array(numeric_labels))\n",
    "    print(f\"\\033[1mMetrics calculated: Alignment = {metrics['alignment']:.4f}, Uniformity = {metrics['uniformity']:.4f}\\033[0m\")\n",
    "\n",
    "    # 3. Perform t-SNE dimensionality reduction\n",
    "    print(\"Running t-SNE dimensionality reduction (this may take some time)...\")\n",
    "    tsne = TSNE(n_components=2, perplexity=30, random_state=42, n_iter=300, init='pca')\n",
    "    tsne_embeddings = tsne.fit_transform(test_embeddings)\n",
    "\n",
    "    # 4. Plot the results\n",
    "    print(\"Plotting results...\")\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(\n",
    "        x=tsne_embeddings[:, 0], \n",
    "        y=tsne_embeddings[:, 1],\n",
    "        hue=numeric_labels,\n",
    "        palette=\"colorblind\",\n",
    "        legend=\"full\",\n",
    "        alpha=0.8\n",
    "    )\n",
    "    plt.title(f't-SNE of Test Set Embeddings (K=100)\\n{experiment_title.replace(\"_\", \" \")}', fontsize=16)\n",
    "    plt.xlabel('t-SNE Dimension 1')\n",
    "    plt.ylabel('t-SNE Dimension 2')\n",
    "    \n",
    "    # Update the legend with the correct class_names\n",
    "    handles, _ = plt.gca().get_legend_handles_labels()\n",
    "    plt.legend(handles=handles, labels=class_names, title='Classes')\n",
    "    \n",
    "    # 5. Save and display the plot\n",
    "    save_path = f'results/{experiment_title}_tsne_K100.png'\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"t-SNE plot saved to: {save_path}\")\n",
    "    plt.show()\n",
    "    print(\"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7705257c-d00c-4257-9f91-f1bfd918776a",
   "metadata": {
    "id": "7705257c-d00c-4257-9f91-f1bfd918776a"
   },
   "outputs": [],
   "source": [
    "# Setup trainer\n",
    "\n",
    "def setup_trainer(model, train_dataset, val_dataset, optimizer, fold_counter, seed_counter):\n",
    "    training_args = get_training_args(fold_counter, seed_counter)\n",
    "\n",
    "    trainer = ContrastiveLearningTrainer(\n",
    "        model = model,\n",
    "        args = training_args,\n",
    "        optimizers = (optimizer, None),\n",
    "        train_dataset = train_dataset,\n",
    "        eval_dataset = val_dataset,\n",
    "        data_collator = data_collator,\n",
    "        compute_metrics = calculate_metrics,\n",
    "        callbacks = CALLBACKS\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Zgik1HRpGevG",
   "metadata": {
    "id": "Zgik1HRpGevG"
   },
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DaGnbrV1a-bE",
   "metadata": {
    "id": "DaGnbrV1a-bE"
   },
   "outputs": [],
   "source": [
    "def get_embedding_bart_encoder(X, y, model=None):\n",
    "    if model == None:\n",
    "        free_gpu()\n",
    "        embedding_model_name = MODEL_NAME \n",
    "        model = load_model(embedding_model_name)\n",
    "        model = model.to(device)\n",
    "\n",
    "    Xy_tokenized = tokenize_inputs(emb_tokenizer, tuple(X), tuple(y))\n",
    "    dataset = get_dataset(Xy_tokenized)\n",
    "    dataloader = get_dataloader(dataset, PER_DEVICE_VAL_BATCH_SIZE_EMBEDDING)\n",
    "\n",
    "    encoder = []\n",
    "    model.eval()\n",
    "    for batch in dataloader:\n",
    "        gpu_batch = {k:v.to(device) for k,v in batch.items() if k != 'labels'}\n",
    "        \n",
    "        if hasattr(model, \"encoder\") and model.config.is_encoder_decoder:\n",
    "\n",
    "            output = model.encoder(**gpu_batch, return_dict=True)\n",
    "            last_hidden_state = output.last_hidden_state\n",
    "        else:\n",
    "            base_model = getattr(model, model.base_model_prefix, model)\n",
    "            output = base_model(**gpu_batch, return_dict=True)\n",
    "            last_hidden_state = output.last_hidden_state\n",
    "\n",
    "        batch_encoder = torch.reshape(\n",
    "            torch.sum(last_hidden_state, 1) / torch.sum(torch.reshape(gpu_batch['attention_mask'], (len(batch['attention_mask']), -1, 1)).expand(last_hidden_state.size()), 1)\n",
    "        , (len(batch['attention_mask']), -1)).to('cpu').detach().numpy()\n",
    "\n",
    "        encoder.append(batch_encoder)\n",
    "        del gpu_batch\n",
    "\n",
    "    model.cpu()\n",
    "    del model\n",
    "    free_gpu()\n",
    "\n",
    "    encoder = np.concatenate(encoder, axis=0)\n",
    "\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BHifUulofn59",
   "metadata": {
    "id": "BHifUulofn59"
   },
   "outputs": [],
   "source": [
    "def get_embedding_bart_scores(X, y, model, get_encoder=False):\n",
    "    if model is None:\n",
    "        raise Exception(f\"`model` is not defined.\")\n",
    "\n",
    "    Xy_tokenized = tokenize_inputs(emb_tokenizer, tuple(X), tuple(y))\n",
    "    dataset = get_dataset(Xy_tokenized)\n",
    "    dataloader = get_dataloader(dataset, PER_DEVICE_VAL_BATCH_SIZE_EMBEDDING)\n",
    "\n",
    "    scores = [] # (len(X), NUM_CLASSES)\n",
    "    encoder = [] # List to store encoder's last hidden states\n",
    "\n",
    "    for batch in dataloader:\n",
    "        gpu_batch = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
    "\n",
    "        # Forward pass through the model\n",
    "        outputs = model(**gpu_batch, output_hidden_states=get_encoder, return_dict=True)\n",
    "\n",
    "        if get_encoder:\n",
    "            encoder.append(\n",
    "                torch.reshape(\n",
    "                    torch.sum(outputs.encoder_hidden_states[-1], 1) /\n",
    "                    torch.sum(\n",
    "                        torch.reshape(\n",
    "                            gpu_batch[\"attention_mask\"],\n",
    "                            (len(batch[\"attention_mask\"]), -1, 1)\n",
    "                        ).expand(outputs.encoder_hidden_states[-1].size()),\n",
    "                        1\n",
    "                    ),\n",
    "                    (len(batch[\"attention_mask\"]), -1)\n",
    "                ).to(\"cpu\").detach().numpy()\n",
    "            )\n",
    "\n",
    "        logits = outputs.logits # (batch_size, seq_len, vocab_size)\n",
    "        logits = logits.cpu().detach().numpy()\n",
    "        sequences = np.argmax(logits, axis=-1) # (batch_size, seq_len)\n",
    "\n",
    "        batch_scores = np.zeros((logits.shape[0], logits.shape[1], NUM_CLASSES)) # (batch_size, seq_len, NUM_CLASSES)\n",
    "        for seq_tokens_i in range(1, logits.shape[1] - 1):\n",
    "            for class_token_i, class_token in enumerate(CLASSES_TOKENS):\n",
    "                batch_scores[:, seq_tokens_i, class_token_i] = logits[:, seq_tokens_i, class_token]\n",
    "            for batch_i in range(logits.shape[0]): # Softmax over class tokens for valid predictions\n",
    "                if sequences[batch_i, seq_tokens_i] in CLASSES_TOKENS:\n",
    "                    batch_scores[batch_i, seq_tokens_i] = softmax(batch_scores[batch_i, seq_tokens_i], axis=-1)\n",
    "                else:\n",
    "                    batch_scores[batch_i, seq_tokens_i] = 0\n",
    "        batch_scores = np.max(batch_scores, axis=1).reshape(-1, NUM_CLASSES) # Reduce seq_len dimension\n",
    "\n",
    "        scores.append(batch_scores)\n",
    "\n",
    "        del gpu_batch\n",
    "\n",
    "    # Convert scores and encoder hidden states to single arrays\n",
    "    scores = np.concatenate(scores, axis=0)\n",
    "    encoder = np.concatenate(encoder, axis=0) if get_encoder else None\n",
    "\n",
    "    if get_encoder:\n",
    "        return scores, encoder\n",
    "    else:\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RUPQwZIQjG0i",
   "metadata": {
    "id": "RUPQwZIQjG0i"
   },
   "outputs": [],
   "source": [
    "def get_embedding_t5_encoder(X, y, model=None, flan=False, prefix=None):\n",
    "    if model == None:\n",
    "        free_gpu()\n",
    "        embedding_model_name = MODEL_NAME \n",
    "        model = load_model(embedding_model_name)\n",
    "        model = model.to(device)\n",
    "\n",
    "    X_with_prefix = X\n",
    "    if prefix is not None:\n",
    "        X_with_prefix = [prefix+x for x in X]\n",
    "\n",
    "    Xy_tokenized = tokenize_inputs(emb_tokenizer, tuple(X_with_prefix), tuple(y))\n",
    "    dataset = get_dataset(Xy_tokenized)\n",
    "    dataloader = get_dataloader(dataset, PER_DEVICE_VAL_BATCH_SIZE_EMBEDDING)\n",
    "\n",
    "    encoder = []\n",
    "    model.eval()\n",
    "    for batch in dataloader:\n",
    "        gpu_batch = {k:v.to(device) for k,v in batch.items() if k != 'labels'}\n",
    "        \n",
    "\n",
    "        if hasattr(model, \"encoder\") and model.config.is_encoder_decoder:\n",
    "\n",
    "            output = model.encoder(**gpu_batch, return_dict=True)\n",
    "            last_hidden_state = output.last_hidden_state\n",
    "        else:\n",
    "            base_model = getattr(model, model.base_model_prefix, model)\n",
    "            output = base_model(**gpu_batch, return_dict=True)\n",
    "            last_hidden_state = output.last_hidden_state\n",
    "\n",
    "        batch_encoder = torch.reshape(\n",
    "            torch.sum(last_hidden_state, 1) / torch.sum(torch.reshape(gpu_batch['attention_mask'], (len(batch['attention_mask']), -1, 1)).expand(last_hidden_state.size()), 1)\n",
    "        , (len(batch['attention_mask']), -1)).to('cpu').detach().numpy()\n",
    "\n",
    "        encoder.append(batch_encoder)\n",
    "        del gpu_batch\n",
    "\n",
    "    model.cpu()\n",
    "    del model\n",
    "    free_gpu()\n",
    "\n",
    "    encoder = np.concatenate(encoder, axis=0)\n",
    "\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZHa5H4BTMWDf",
   "metadata": {
    "id": "ZHa5H4BTMWDf"
   },
   "outputs": [],
   "source": [
    "def get_embedding_t5_scores(X, y, model, get_encoder=False):\n",
    "    if model is None:\n",
    "        raise Exception(f'`model` is not defined.')\n",
    "\n",
    "    Xy_tokenized = tokenize_inputs(tokenizer, tuple(X), tuple(y))\n",
    "    dataset = get_dataset(Xy_tokenized)\n",
    "    dataloader = get_dataloader(dataset, PER_DEVICE_VAL_BATCH_SIZE_EMBEDDING)\n",
    "\n",
    "    scores = []  # (len(X), NUM_CLASSES)\n",
    "    encoder = []  # List to store the encoder's last hidden states\n",
    "\n",
    "    for batch in dataloader:\n",
    "        gpu_batch = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "\n",
    "        output = model.generate(**gpu_batch, return_dict_in_generate=True, output_scores=True, max_new_tokens=MAX_NEW_TOKENS, renormalize_logits=True, output_hidden_states=get_encoder)\n",
    "\n",
    "        if get_encoder:\n",
    "            encoder.append(\n",
    "                torch.reshape(\n",
    "                    torch.sum(output.encoder_hidden_states[-1], 1) /\n",
    "                    torch.sum(\n",
    "                        torch.reshape(\n",
    "                            gpu_batch[\"attention_mask\"],\n",
    "                            (len(batch[\"attention_mask\"]), -1, 1)\n",
    "                        ).expand(output.encoder_hidden_states[-1].size()),\n",
    "                        1\n",
    "                    ),\n",
    "                    (len(batch[\"attention_mask\"]), -1)\n",
    "                ).to(\"cpu\").detach().numpy()\n",
    "            )\n",
    "\n",
    "        outputsequences = output.sequences.cpu().detach().numpy()\n",
    "        outputscores = np.array([score.cpu().detach().numpy() for score in output.scores])\n",
    "        outputscores = outputscores.transpose([1, 0, 2]) # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "        batch_scores = np.zeros((outputscores.shape[0], outputscores.shape[1], NUM_CLASSES)) # (batch_size, seq_len, NUM_CLASSES)\n",
    "        for seq_tokens_i in range(outputscores.shape[1]-1):\n",
    "            for class_token_i, class_token in enumerate(CLASSES_TOKENS):\n",
    "                batch_scores[:, seq_tokens_i, class_token_i] = outputscores[:, seq_tokens_i, class_token]\n",
    "            for batch_i in range(outputscores.shape[0]): # Ignore non class tokens, and softmax over class tokens\n",
    "                if outputsequences[batch_i, seq_tokens_i+1] in CLASSES_TOKENS:\n",
    "                    batch_scores[batch_i, seq_tokens_i] = softmax(batch_scores[batch_i, seq_tokens_i], axis=-1)\n",
    "                else:\n",
    "                    batch_scores[batch_i, seq_tokens_i] = 0\n",
    "        batch_scores = np.max(batch_scores, axis=1).reshape(-1, NUM_CLASSES) # (batch_size, NUM_CLASSES) # This is especially important for multi-label tasks\n",
    "\n",
    "        scores.append(batch_scores)\n",
    "\n",
    "        del gpu_batch\n",
    "\n",
    "    # Convert scores and encoder hidden states to single arrays\n",
    "    scores = np.concatenate(scores, axis=0)\n",
    "    encoder = np.concatenate(encoder, axis=0) if get_encoder else None\n",
    "\n",
    "    if get_encoder:\n",
    "        return scores, encoder\n",
    "    else:\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64641fd8-27fb-4618-94d7-1e2a24f968a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_classification_scores(X, y, model):\n",
    "    if model is None:\n",
    "        raise Exception(\"`model` is not defined.\")\n",
    "\n",
    "    Xy_tokenized = tokenize_inputs(tokenizer, tuple(X), tuple(y))\n",
    "    dataset = get_dataset(Xy_tokenized)\n",
    "    dataloader = get_dataloader(dataset, PER_DEVICE_VAL_BATCH_SIZE_EMBEDDING)\n",
    "\n",
    "    scores = [] \n",
    "    model.eval()\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        gpu_batch = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**gpu_batch)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        scores.append(probs.cpu().detach().numpy())\n",
    "        del gpu_batch\n",
    "\n",
    "    return np.concatenate(scores, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CZ79ZspjaBJm",
   "metadata": {
    "id": "CZ79ZspjaBJm"
   },
   "outputs": [],
   "source": [
    "def get_embedding(X, y, embedding_method=EMBEDDING_METHOD_FIRST_ITERATION, model=None):\n",
    "    with torch.no_grad():\n",
    "        if embedding_method == 'BART-ENCODER':\n",
    "            return get_embedding_bart_encoder(X, y, model)\n",
    "        elif embedding_method == 'BART-SCORES':\n",
    "            return get_embedding_bart_scores(X, y, model)\n",
    "        elif embedding_method == 'BART-SCORES+ENCODER':\n",
    "            return get_embedding_bart_scores(X, y, model, get_encoder=True)\n",
    "        elif embedding_method == 'T5-ENCODER':\n",
    "            return get_embedding_t5_encoder(X, y, model=model, flan=False, prefix=EMBEDDING_PREFIX)\n",
    "        elif embedding_method == 'FLAN-T5-ENCODER':\n",
    "            return get_embedding_t5_encoder(X, y, model=model, flan=True, prefix=EMBEDDING_PREFIX)\n",
    "        elif embedding_method == 'FLAN-T5-SCORES':\n",
    "            return get_embedding_t5_scores(X, y, model)\n",
    "        elif embedding_method == 'FLAN-T5-SCORES+ENCODER':\n",
    "            return get_embedding_t5_scores(X, y, model, get_encoder=True)\n",
    "        elif embedding_method == 'CLASSIFICATION-SCORES':\n",
    "            return get_embedding_classification_scores(X, y, model)\n",
    "        else:\n",
    "            raise Exception(f\"The embedding method `{embedding_method}` is not defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3W_fUcyiGjyk",
   "metadata": {
    "id": "3W_fUcyiGjyk"
   },
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ygllI3PXYz3Q",
   "metadata": {
    "id": "ygllI3PXYz3Q"
   },
   "outputs": [],
   "source": [
    "def sample_balanced(X, y, number_of_samples=NUMBER_OF_SAMPLES_PER_ITERATION[0], seed=0):\n",
    "    seen = [False] * len(X) # Flag used for not including already seen samples twice.\n",
    "    sampled_X = []\n",
    "    sampled_y = []\n",
    "    rest_X    = []\n",
    "    rest_y    = []\n",
    "\n",
    "    for class_name in CLASSES:\n",
    "        partial_X = []\n",
    "        partial_y = []\n",
    "        for i, (ax, ay) in enumerate(zip(X, y)):\n",
    "            if seen[i] == False and class_name in ay: # May include 1+ types in MPQA-T task because of using 'in' keyword instead of '=='.\n",
    "                seen[i] = True\n",
    "                partial_X.append(ax)\n",
    "                partial_y.append(ay)\n",
    "\n",
    "        sampled_ids = sample_without_replacement(len(partial_X), number_of_samples, random_state=seed)\n",
    "        for id in range(len(partial_X)):\n",
    "            if id in sampled_ids:\n",
    "                sampled_X.append(partial_X[id])\n",
    "                sampled_y.append(partial_y[id])\n",
    "            else:\n",
    "                rest_X.append(partial_X[id])\n",
    "                rest_y.append(partial_y[id])\n",
    "\n",
    "    return sampled_X, sampled_y, rest_X, rest_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88g_UVW8xFsv",
   "metadata": {
    "id": "88g_UVW8xFsv"
   },
   "outputs": [],
   "source": [
    "def sample_random(X, y, number_of_samples=NUMBER_OF_SAMPLES_PER_ITERATION[0], seed=0):\n",
    "    sampled_X = []\n",
    "    sampled_y = []\n",
    "    rest_X    = []\n",
    "    rest_y    = []\n",
    "\n",
    "    sampled_ids = sample_without_replacement(len(X), number_of_samples, random_state=seed)\n",
    "    for id in range(len(X)):\n",
    "        if id in sampled_ids:\n",
    "            sampled_X.append(X[id])\n",
    "            sampled_y.append(y[id])\n",
    "        else:\n",
    "            rest_X.append(X[id])\n",
    "            rest_y.append(y[id])\n",
    "\n",
    "    return sampled_X, sampled_y, rest_X, rest_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CXG2jrCEb47N",
   "metadata": {
    "id": "CXG2jrCEb47N"
   },
   "outputs": [],
   "source": [
    "def sample_kmeans_random(X, y, emb, number_of_samples=NUMBER_OF_SAMPLES_PER_ITERATION[0], seed=0):\n",
    "    km = KMeans(n_clusters=number_of_samples, init=KMEANS_INIT, n_init=KMEANS_CENTROID_SEEDS, max_iter=300, random_state=seed, algorithm=KMEANS_ALGORITHM)\n",
    "    km.fit(emb)\n",
    "    centers = np.array(km.cluster_centers_)\n",
    "    cluster_ids, cluster_sizes = np.unique(km.labels_, return_counts=True)\n",
    "    print(f\"Number of elements assigned to each cluster: {cluster_sizes}\")\n",
    "\n",
    "    sampled_X = []\n",
    "    sampled_y = []\n",
    "    rest_X = []\n",
    "    rest_y = []\n",
    "\n",
    "    for cluster_id in cluster_ids:\n",
    "        cluster_emb = emb[km.labels_ == cluster_id]\n",
    "        selected_ids = np.arange(len(emb))[km.labels_ == cluster_id]\n",
    "        sampled_ids = sample_without_replacement(len(cluster_emb), 1, random_state=seed)\n",
    "        sampled_id = sampled_ids[0]\n",
    "        sampled_X.append(X[selected_ids[sampled_id]])\n",
    "        sampled_y.append(y[selected_ids[sampled_id]])\n",
    "        for i, id in enumerate(selected_ids):\n",
    "            if i != sampled_id:\n",
    "                rest_X.append(X[id])\n",
    "                rest_y.append(y[id])\n",
    "\n",
    "    return sampled_X, sampled_y, rest_X, rest_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xrLcABxUUOb7",
   "metadata": {
    "id": "xrLcABxUUOb7"
   },
   "outputs": [],
   "source": [
    "def sample_kmeans_representative(X, y, emb, number_of_samples=NUMBER_OF_SAMPLES_PER_ITERATION[0], seed=0):\n",
    "    km = KMeans(n_clusters=number_of_samples, init=KMEANS_INIT, n_init=KMEANS_CENTROID_SEEDS, max_iter=300, random_state=seed, algorithm=KMEANS_ALGORITHM)\n",
    "    km.fit(emb)\n",
    "    centers = np.array(km.cluster_centers_)\n",
    "    cluster_ids, cluster_sizes = np.unique(km.labels_, return_counts=True)\n",
    "    print(f\"Number of elements assigned to each cluster: {cluster_sizes}\")\n",
    "\n",
    "    sampled_X = []\n",
    "    sampled_y = []\n",
    "    rest_X = []\n",
    "    rest_y = []\n",
    "\n",
    "    for cluster_id in cluster_ids:\n",
    "        cluster_emb = emb[km.labels_ == cluster_id]\n",
    "        selected_ids = np.arange(len(emb))[km.labels_ == cluster_id]\n",
    "        closests, distances = pairwise_distances_argmin_min(centers[cluster_id:cluster_id+1], cluster_emb)\n",
    "        closest = closests[0]\n",
    "        sampled_X.append(X[selected_ids[closest]])\n",
    "        sampled_y.append(y[selected_ids[closest]])\n",
    "        for i, id in enumerate(selected_ids):\n",
    "            if i != closest:\n",
    "                rest_X.append(X[id])\n",
    "                rest_y.append(y[id])\n",
    "\n",
    "    return sampled_X, sampled_y, rest_X, rest_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ue2cjoZ3QTTs",
   "metadata": {
    "id": "ue2cjoZ3QTTs"
   },
   "outputs": [],
   "source": [
    "def sample_entropy(X, y, emb, number_of_samples=NUMBER_OF_SAMPLES_PER_ITERATION, seed=0, encoder_emb=None):\n",
    "    entropies = - np.sum(emb * np.log(emb + 1e-9), axis=1)\n",
    "    sorted_ids = np.argsort(entropies)\n",
    "    sampled_ids = sorted_ids[-number_of_samples:]\n",
    "\n",
    "    sampled_encoder_emb = []\n",
    "    sampled_X = []\n",
    "    sampled_y = []\n",
    "    rest_X = []\n",
    "    rest_y = []\n",
    "\n",
    "    for id in range(len(X)):\n",
    "        if id in sampled_ids:\n",
    "            if encoder_emb is not None:\n",
    "                sampled_encoder_emb.append(encoder_emb[id])\n",
    "            sampled_X.append(X[id])\n",
    "            sampled_y.append(y[id])\n",
    "        else:\n",
    "            rest_X.append(X[id])\n",
    "            rest_y.append(y[id])\n",
    "\n",
    "    if encoder_emb is not None:\n",
    "        return sampled_X, sampled_y, rest_X, rest_y, np.array(sampled_encoder_emb)\n",
    "\n",
    "    return sampled_X, sampled_y, rest_X, rest_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4spfylXENylR",
   "metadata": {
    "id": "4spfylXENylR"
   },
   "outputs": [],
   "source": [
    "def sample_entropy_then_kmeans_representative(X, y, emb, number_of_samples=NUMBER_OF_SAMPLES_PER_ITERATION[0], seed=0):\n",
    "\n",
    "    if type(emb).__name__ == 'tuple':\n",
    "        kmeans_emb = emb[1] # Encoder\n",
    "        entropy_emb = emb[0] # Scores\n",
    "    else:\n",
    "        kmeans_emb = emb # Scores\n",
    "        entropy_emb = emb # Scores\n",
    "\n",
    "    sampled_X_1, sampled_y_1, rest_X_1, rest_y_1, sampled_encoder_emb = sample_entropy(X, y, entropy_emb, number_of_samples*UNREP_ALPHA, seed, encoder_emb=kmeans_emb)\n",
    "    sampled_X_2, sampled_y_2, rest_X_2, rest_y_2 = sample_kmeans_representative(sampled_X_1, sampled_y_1, sampled_encoder_emb, number_of_samples, seed)\n",
    "\n",
    "    sampled_X = sampled_X_2\n",
    "    sampled_y = sampled_y_2\n",
    "    rest_X = rest_X_1 + rest_X_2\n",
    "    rest_y = rest_y_1 + rest_y_2\n",
    "\n",
    "    return sampled_X, sampled_y, rest_X, rest_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qHjNZXzzPgHZ",
   "metadata": {
    "id": "qHjNZXzzPgHZ"
   },
   "outputs": [],
   "source": [
    "def sample_kmeans_representative_then_entropy(X, y, emb, number_of_samples=NUMBER_OF_SAMPLES_PER_ITERATION[0], seed=0):\n",
    "\n",
    "    if type(emb).__name__ == 'tuple':\n",
    "        kmeans_emb = emb[1] # Encoder\n",
    "        entropy_emb = emb[0] # Scores\n",
    "    else:\n",
    "        kmeans_emb = emb # Scores\n",
    "        entropy_emb = emb # Scores\n",
    "\n",
    "    km = KMeans(n_clusters=number_of_samples, init=KMEANS_INIT, n_init=KMEANS_CENTROID_SEEDS, max_iter=300, random_state=seed, algorithm=KMEANS_ALGORITHM)\n",
    "    km.fit(kmeans_emb)\n",
    "    centers = np.array(km.cluster_centers_)\n",
    "    cluster_ids, cluster_sizes = np.unique(km.labels_, return_counts=True)\n",
    "    print(f\"Number of elements assigned to each cluster: {cluster_sizes}\")\n",
    "\n",
    "    sampled_X = []\n",
    "    sampled_y = []\n",
    "    rest_X = []\n",
    "    rest_y = []\n",
    "\n",
    "    for cluster_id in cluster_ids:\n",
    "\n",
    "        cluster_entropy_emb = entropy_emb[km.labels_ == cluster_id]\n",
    "        selected_ids = np.arange(len(entropy_emb))[km.labels_ == cluster_id]\n",
    "        cluster_X = []\n",
    "        cluster_y = []\n",
    "        for id in selected_ids:\n",
    "            cluster_X.append(X[id])\n",
    "            cluster_y.append(y[id])\n",
    "\n",
    "        one_sampled_X, one_sampled_y, rest_cluster_X, rest_cluster_y = sample_entropy(cluster_X, cluster_y, cluster_entropy_emb, 1, seed)\n",
    "\n",
    "        sampled_X += one_sampled_X\n",
    "        sampled_y += one_sampled_y\n",
    "        rest_X += rest_cluster_X\n",
    "        rest_y += rest_cluster_y\n",
    "\n",
    "    return sampled_X, sampled_y, rest_X, rest_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4w_K1sG2wdfx",
   "metadata": {
    "id": "4w_K1sG2wdfx"
   },
   "outputs": [],
   "source": [
    "def sample(X, y, emb=None, sampling_method=SAMPLING_METHOD_FIRST_ITERATION, number_of_samples=NUMBER_OF_SAMPLES_PER_ITERATION[0], seed=0):\n",
    "    if sampling_method == 'BALANCED':\n",
    "        return sample_balanced(X, y, number_of_samples, seed)\n",
    "    elif sampling_method == 'RANDOM':\n",
    "        return sample_random(X, y, number_of_samples, seed)\n",
    "    elif sampling_method == 'KMEANS_RANDOM':\n",
    "        return sample_kmeans_random(X, y, emb, number_of_samples, seed)\n",
    "    elif sampling_method == 'KMEANS_REPRESENTATIVE':\n",
    "        return sample_kmeans_representative(X, y, emb, number_of_samples, seed)\n",
    "    elif sampling_method == 'ENTROPY':\n",
    "        return sample_entropy(X, y, emb, number_of_samples, seed)\n",
    "    elif sampling_method == 'ENTROPY_THEN_KMEANS_REPRESENTATIVE':\n",
    "        return sample_entropy_then_kmeans_representative(X, y, emb, number_of_samples, seed)\n",
    "    elif sampling_method == 'KMEANS_REPRESENTATIVE_THEN_ENTROPY':\n",
    "        return sample_kmeans_representative_then_entropy(X, y, emb, number_of_samples, seed)\n",
    "    else:\n",
    "        raise Exception(f\"The sampling method `{sampling_method}` is not defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O2FIRYcgGnVK",
   "metadata": {
    "id": "O2FIRYcgGnVK"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ec8jLHChPuH3",
   "metadata": {
    "id": "Ec8jLHChPuH3"
   },
   "outputs": [],
   "source": [
    "%mkdir results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n1F5NUhkJ2bf",
   "metadata": {
    "id": "n1F5NUhkJ2bf"
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "folds_val_log          = INIT_FOLDS_VAL_LOG\n",
    "folds_test_log         = INIT_FOLDS_TEST_LOG\n",
    "preds_texts_val_list   = INIT_PREDS_TEXTS_VAL_LIST\n",
    "targets_texts_val_list = INIT_TARGETS_TEXTS_VAL_LIST\n",
    "sampled_list           = INIT_SAMPLED_LIST\n",
    "duration_log = ''\n",
    "\n",
    "for fold_counter in range(MAX_FOLDS):\n",
    "    print(f'\\\\n\\\\033[1mFold {fold_counter}/{MAX_FOLDS-1}:\\\\033[0m')\n",
    "\n",
    "    for seed_counter in range(SKIP_SAMPLING_SEEDS, REPEAT_SAMPLING):\n",
    "        print(f'\\\\n\\\\033[1mFold {fold_counter}/{MAX_FOLDS-1} | Seed {seed_counter}/{REPEAT_SAMPLING-1}:\\\\033[0m')\n",
    "\n",
    "        set_seed()\n",
    "\n",
    "\n",
    "        current_learning_rate = INITIAL_LEARNING_RATE\n",
    "\n",
    "        folds_val_log[-1].append([])\n",
    "        folds_test_log[-1].append([])\n",
    "        preds_texts_val_list[-1].append([])\n",
    "        sampled_list[-1].append([])\n",
    "\n",
    "        model=None\n",
    "        (X_train_rest, y_train_rest), (X_val, y_val), (X_test, y_test) = fetch_dataset(dataset_name=DATASET_NAME, fold=fold_counter, show_counter=(seed_counter==0))\n",
    "        X_train, y_train = [], []\n",
    "\n",
    "        for sampling_iteration in range(SAMPLING_ITERATIONS):\n",
    "            print(f'\\\\n\\\\033[1mFold {fold_counter}/{MAX_FOLDS-1} | Seed {seed_counter}/{REPEAT_SAMPLING-1} | Iteration {sampling_iteration}/{SAMPLING_ITERATIONS-1}:\\\\033[0m')\n",
    "            duration_log += f'Fold {fold_counter}/{MAX_FOLDS-1} | Seed {seed_counter}/{REPEAT_SAMPLING-1} | Iteration {sampling_iteration}/{SAMPLING_ITERATIONS-1}:\\\\n'\n",
    "\n",
    "            logging.disable(logging.INFO)\n",
    "\n",
    "            embedding_method = EMBEDDING_METHOD_FIRST_ITERATION if sampling_iteration == 0 or EMBEDDING_METHOD_SECOND_ITERATION_PLUS == \"SAME_AS_BEFORE\" else EMBEDDING_METHOD_SECOND_ITERATION_PLUS\n",
    "            sampling_method  = SAMPLING_METHOD_FIRST_ITERATION  if sampling_iteration == 0 or SAMPLING_METHOD_SECOND_ITERATION_PLUS  == \"SAME_AS_BEFORE\" else SAMPLING_METHOD_SECOND_ITERATION_PLUS\n",
    "\n",
    "            embedding_start_time = datetime.now()\n",
    "            if sampling_method in ['RANDOM', 'BALANCED']:\n",
    "                emb_train = None\n",
    "            else:\n",
    "                emb_train = get_embedding(X_train_rest, y_train_rest, embedding_method=embedding_method, model=model)\n",
    "            duration_log += f'Embedding Time: {datetime.now()-embedding_start_time}\\\\n'\n",
    "\n",
    "            sampling_start_time = datetime.now()\n",
    "            X_train_sampled, y_train_sampled, X_train_rest, y_train_rest = sample(X_train_rest, y_train_rest, emb_train, sampling_method=sampling_method, number_of_samples=NUMBER_OF_SAMPLES_PER_ITERATION[sampling_iteration], seed=seed_counter)\n",
    "            duration_log += f'Sampling Time: {datetime.now()-sampling_start_time}\\\\n'\n",
    "\n",
    "            if AFL_APPROACH == 'IN-CONTEXT':\n",
    "                X_train_sampled = [x.splitlines()[-1] for x in X_train_sampled]\n",
    "                support_set_context_sampled = ''\n",
    "                for x, y in zip(X_train_sampled, y_train_sampled):\n",
    "                    support_set_context_sampled += x + ' ' + y + '\\\\n'\n",
    "                X_train_rest = [support_set_context_sampled + x for x in X_train_rest]\n",
    "                X_val        = [support_set_context_sampled + x for x in X_val]\n",
    "                X_test       = [support_set_context_sampled + x for x in X_test]\n",
    "\n",
    "            sampled_list[-1][-1].append((X_train_sampled, y_train_sampled))\n",
    "\n",
    "            X_train += X_train_sampled\n",
    "            y_train += y_train_sampled\n",
    "\n",
    "            Xy_train_tokenized = tokenize_inputs(tokenizer, tuple(X_train), tuple(y_train)) if seed_counter != 0 else tokenize_and_show_inputs(tokenizer, X_train, y_train)\n",
    "            Xy_val_tokenized   = tokenize_inputs(tokenizer, tuple(X_val),   tuple(y_val))\n",
    "            Xy_test_tokenized  = tokenize_inputs(tokenizer, tuple(X_test),  tuple(y_test))\n",
    "\n",
    "            n_train_samples = len(X_train)\n",
    "            n_val_samples   = len(X_val)\n",
    "            n_test_samples  = len(X_test)\n",
    "\n",
    "            n_samples = n_train_samples + n_val_samples + n_test_samples\n",
    "            if seed_counter == 0:\n",
    "                print(f'Train set size:      {n_train_samples} \\\\t({100*n_train_samples/n_samples}%)')\n",
    "                print(f'Validation set size: {n_val_samples} \\\\t({100*n_val_samples/n_samples}%)')\n",
    "                print(f'Test set size:       {n_test_samples} \\\\t({100*n_test_samples/n_samples}%)')\n",
    "                print()\n",
    "\n",
    "            train_dataset = get_dataset(Xy_train_tokenized)\n",
    "            val_dataset   = get_dataset(Xy_val_tokenized)\n",
    "            test_dataset  = get_dataset(Xy_test_tokenized)\n",
    "\n",
    "            if sampling_iteration == 0 or RESET_MODEL_AFTER_EACH_ITERATION:\n",
    "                print(\"\\\\033[1;31mLoading/Resetting model from scratch.\\\\033[0m\")\n",
    "                free_gpu()\n",
    "                model = load_model(MODEL_NAME)\n",
    "                model = model.to(device)\n",
    "            else:\n",
    "                current_learning_rate *= LEARNING_RATE_DECAY\n",
    "                print(f\"\\\\033[1;32mContinuing training on existing model. New learning rate: {current_learning_rate}\\\\033[0m\")\n",
    "            \n",
    "            LEARNING_RATE = current_learning_rate\n",
    "\n",
    "            optimizer = load_optimizer(model)\n",
    "            X_eval, y_eval = X_val, y_val\n",
    "            %mkdir models\n",
    "            trainer = setup_trainer(model, train_dataset, val_dataset, optimizer, fold_counter, seed_counter)\n",
    "\n",
    "            if AFL_APPROACH == \"FINE-TUNING\":\n",
    "                ft_start_time = datetime.now()\n",
    "\n",
    "                trainer.train()\n",
    "                duration_log += f'Fine-Tuning Time: {datetime.now()-ft_start_time}\\\\n'\n",
    "\n",
    "            X_eval, y_eval = X_val, y_val\n",
    "            val_results  = trainer.evaluate(val_dataset)\n",
    "            X_eval, y_eval = X_test, y_test\n",
    "            test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "            if AFL_APPROACH == \"FINE-TUNING\":\n",
    "                if os.path.exists(f'models/{EXPERIMENT_NAME}_{fold_counter}_{seed_counter}'):\n",
    "                    checkpoints = [d for d in os.listdir(f'models/{EXPERIMENT_NAME}_{fold_counter}_{seed_counter}') if d.startswith('checkpoint')]\n",
    "                    if checkpoints:\n",
    "                        best_epoch_num = max([int(c.split('-')[-1]) for c in checkpoints])\n",
    "                        best_epoch = best_epoch_num * PER_DEVICE_TRAIN_BATCH_SIZE / n_train_samples\n",
    "                        val_results['best_epoch'] = best_epoch\n",
    "                        test_results['best_epoch'] = best_epoch\n",
    "            elif AFL_APPROACH == \"IN-CONTEXT\":\n",
    "                val_results['best_epoch'] = 0\n",
    "                test_results['best_epoch'] = 0\n",
    "            %rm -r models\n",
    "\n",
    "            folds_val_log[-1][-1].append(val_results)\n",
    "            folds_test_log[-1][-1].append(test_results)\n",
    "\n",
    "            X_eval, y_eval = X_val, y_val\n",
    "            pred_val = trainer.predict(val_dataset)\n",
    "            preds_tokens_val = pred_val.predictions[0] if isinstance(pred_val.predictions, tuple) else pred_val.predictions\n",
    "            if preds_tokens_val.dtype in [np.float32, np.float64, float]:\n",
    "                preds_indices_val = np.argmax(preds_tokens_val, axis=-1)\n",
    "                preds_texts_val = [CLASS_IDS[int(idx)] for idx in preds_indices_val]\n",
    "            else:\n",
    "                preds_texts_val = batch_detokenize(tokenizer, preds_tokens_val)\n",
    "            preds_texts_val_list[-1][-1].append(preds_texts_val)\n",
    "\n",
    "            logging.disable(logging.NOTSET)\n",
    "\n",
    "        plot_final_tsne(\n",
    "            trainer=trainer, \n",
    "            X_test_data=X_test, \n",
    "            y_test_data=y_test, \n",
    "            total_samples_k=len(y_train),\n",
    "            experiment_title=f\"{EXPERIMENT_NAME}_seed{seed_counter}\"\n",
    "        )\n",
    "\n",
    "        print(\"======== CHECKPOINT ========\")\n",
    "        checkpoint = {\n",
    "            'INIT_FOLDS_VAL_LOG': folds_val_log,\n",
    "            'INIT_FOLDS_TEST_LOG': folds_test_log,\n",
    "            'INIT_PREDS_TEXTS_VAL_LIST': preds_texts_val_list,\n",
    "            'INIT_SAMPLED_LIST': sampled_list\n",
    "        }\n",
    "        with open(f'results/{EXPERIMENT_NAME.replace(\".ipynb\", \"\")}_checkpoint.json', 'w', encoding ='utf8') as checkpoint_file:\n",
    "            json.dump(checkpoint, checkpoint_file, ensure_ascii=False, indent=1)\n",
    "\n",
    "    targets_tokens_val = np.array(pred_val.label_ids, dtype=int)\n",
    "    targets_texts_val = batch_detokenize(tokenizer, targets_tokens_val)\n",
    "    targets_texts_val_list.append(targets_texts_val)\n",
    "    print(f\"INIT_TARGETS_TEXTS_VAL_LIST = {targets_texts_val_list},\")\n",
    "\n",
    "    if fold_counter < MAX_FOLDS - 1:\n",
    "        folds_val_log.append([])\n",
    "        folds_test_log.append([])\n",
    "        preds_texts_val_list.append([])\n",
    "        sampled_list.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6vXgpwu2J2bf",
   "metadata": {
    "id": "6vXgpwu2J2bf"
   },
   "outputs": [],
   "source": [
    "print(\"folds_val_log  =\", folds_val_log)\n",
    "print(\"folds_test_log =\", folds_test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vxQIgYrmgjIT",
   "metadata": {
    "id": "vxQIgYrmgjIT"
   },
   "outputs": [],
   "source": [
    "with open(f'results/{EXPERIMENT_NAME.replace(\".ipynb\", \"\")}_duration.log', 'w') as f:\n",
    "    f.write(duration_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oi5swcph39OC",
   "metadata": {
    "id": "oi5swcph39OC"
   },
   "outputs": [],
   "source": [
    "dataset = get_dataset(Xy_train_tokenized)\n",
    "dataloader = get_dataloader(dataset, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fQg6_JBI5CZg",
   "metadata": {
    "id": "fQg6_JBI5CZg"
   },
   "outputs": [],
   "source": [
    "# --- Cell 82: Model Inference and Device Synchronization ---\n",
    "\n",
    "# Ensure the model is loaded and moved to the correct device\n",
    "if 'model' in locals():\n",
    "    # Comparing direct device objects is more reliable than .type\n",
    "    if model.device != device:\n",
    "        model.to(device)\n",
    "else:\n",
    "    print(\"Error: Model not initialized. Please run the preceding cells to load the model.\")\n",
    "    # Optional: model = load_model(MODEL_NAME).to(device)\n",
    "\n",
    "print(f\"Starting model generation on device: {model.device}...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    scores = None  # Expected shape: (len(X), NUM_CLASSES)\n",
    "    \n",
    "    try:\n",
    "        # Fetch a single batch from the dataloader\n",
    "        batch = next(iter(dataloader))\n",
    "        \n",
    "        gpu_batch = {k: v.to(device) for k, v in batch.items() if hasattr(v, 'to')}\n",
    "        \n",
    "\n",
    "        if model.config.is_encoder_decoder:\n",
    "            output = model.generate(\n",
    "                input_ids=gpu_batch['input_ids'], \n",
    "                attention_mask=gpu_batch['attention_mask'],\n",
    "                return_dict_in_generate=True, \n",
    "                output_scores=True, \n",
    "                max_new_tokens=MAX_NEW_TOKENS, \n",
    "                renormalize_logits=True\n",
    "            )\n",
    "            print(\"Inference successful for Seq2Seq model.\")\n",
    "            \n",
    "            if output.scores:\n",
    "                min_logit_val = torch.min(output.scores[0]).item()\n",
    "                print(f\"First token - Minimum score: {min_logit_val:.4f}\")\n",
    "        else:\n",
    "            output = model(\n",
    "                input_ids=gpu_batch['input_ids'], \n",
    "                attention_mask=gpu_batch['attention_mask']\n",
    "            )\n",
    "            print(\"Forward pass successful for classification model.\")\n",
    "            print(f\"Logits shape: {output.logits.shape}\")\n",
    "        \n",
    "        print(\"Generation completed successfully.\")\n",
    "\n",
    "        if output.scores:\n",
    "            min_score = torch.min(output.scores[0]).item()\n",
    "            print(f\"First token - Minimum score: {min_score:.4f}\")\n",
    "\n",
    "    except StopIteration:\n",
    "        print(\"Dataloader Error: No data found. Please check your dataset/iterator.\")\n",
    "    except KeyError as e:\n",
    "        print(f\"Data Error: Expected key {e} not found in the batch dictionary.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Runtime Error during generation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CVGgVFaFY4sN",
   "metadata": {
    "id": "CVGgVFaFY4sN"
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j5lk3DzPJ2bf",
   "metadata": {
    "id": "j5lk3DzPJ2bf"
   },
   "outputs": [],
   "source": [
    "def print_log(folds_log, portion):\n",
    "    if portion == 'Train':\n",
    "        color = '\\033[1;31m'\n",
    "    elif portion == 'Validation':\n",
    "        color = '\\033[1;32m'\n",
    "    elif portion == 'Test':\n",
    "        color = '\\033[1;36m'\n",
    "\n",
    "    ignored_metrics = ['eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch', 'best_epoch']\n",
    "    highlighted_metrics = ['eval_bleu_score', 'eval_type_micro_f1', 'eval_polarity_accuracy', 'eval_intensity_accuracy', 'eval_average_of_metrics']\n",
    "\n",
    "    print(f'{color}{portion}:\\033[0m')\n",
    "    print(f'\\033[1mBased on steps with best {METRIC_FOR_BEST_MODEL}\\033[0m')\n",
    "\n",
    "    for metric_name in folds_log[0][0][-1].keys():\n",
    "        if metric_name in ignored_metrics:\n",
    "            continue\n",
    "        metric_values_in_each_fold = []\n",
    "        for fold_log in folds_log:\n",
    "            metric_values_in_each_fold.append(np.round([seed_log[-1][metric_name] for seed_log in fold_log], 6).tolist())\n",
    "        average_metric_values_in_each_fold = np.round(np.mean(metric_values_in_each_fold, axis=1), 6)\n",
    "        average = np.round(np.mean(average_metric_values_in_each_fold, axis=0), 6)\n",
    "        print()\n",
    "        if metric_name in highlighted_metrics:\n",
    "            print(f'\\033[1m{metric_name}:\\033[0m {metric_values_in_each_fold}')\n",
    "            print(f'\\033[1meach fold:\\033[0m {average_metric_values_in_each_fold}\\033[0m')\n",
    "            print(f'\\033[1maverage:  \\033[0m {color}{average}\\033[0m')\n",
    "        else:\n",
    "            print(f'\\033[1m{metric_name}:\\033[0m {metric_values_in_each_fold}')\n",
    "            print(f'\\033[1meach fold:\\033[0m {average_metric_values_in_each_fold}\\033[0m')\n",
    "            print(f'\\033[1maverage:\\033[0m {average}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unugBZ_1aVqe",
   "metadata": {
    "id": "unugBZ_1aVqe"
   },
   "outputs": [],
   "source": [
    "# print_log(folds_train_log, 'Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LLbvbz9pgajZ",
   "metadata": {
    "id": "LLbvbz9pgajZ"
   },
   "outputs": [],
   "source": [
    "print_log(folds_val_log, 'Validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ElfTO1c3Enmi",
   "metadata": {
    "id": "ElfTO1c3Enmi"
   },
   "outputs": [],
   "source": [
    "print_log(folds_test_log, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sQWxkAhRcIVM",
   "metadata": {
    "id": "sQWxkAhRcIVM"
   },
   "outputs": [],
   "source": [
    "print('\\033[1m Early Stopping: \\033[0m', end = '')\n",
    "print(f'{np.round(EARLY_STOPPING, 2)} \\n', end='')\n",
    "\n",
    "print('\\033[1m Epochs: \\033[0m', end = '')\n",
    "epochs_in_each_fold = []\n",
    "for fold_log in folds_val_log:\n",
    "    epochs_in_each_fold.append(np.round([seed_log[-1]['epoch'] for seed_log in fold_log], 2).tolist())\n",
    "average_epochs_in_each_fold = np.round(np.mean(epochs_in_each_fold, axis=0), 2)\n",
    "average = np.round(np.mean(average_epochs_in_each_fold, axis=0), 2)\n",
    "print(f'{average} ', end='')\n",
    "print(f'  \\t\\t\\t\\t\\t\\t raw: {epochs_in_each_fold}')\n",
    "\n",
    "print('\\033[1m Epoch in Which the Best Results Were Achieved: \\033[0m', end = '')\n",
    "best_epoch_in_each_fold = []\n",
    "for fold_log in folds_val_log:\n",
    "    best_epoch_in_each_fold.append(np.round([seed_log[-1]['best_epoch'] for seed_log in fold_log], 2).tolist())\n",
    "average_best_epoch_in_each_fold = np.round(np.mean(best_epoch_in_each_fold, axis=0), 2)\n",
    "average = np.round(np.mean(average_best_epoch_in_each_fold, axis=0), 2)\n",
    "print(f'{average} ', end='')\n",
    "print(f'  \\t raw: {best_epoch_in_each_fold}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oumTpyqnJ2bh",
   "metadata": {
    "id": "oumTpyqnJ2bh"
   },
   "source": [
    "# Save/Load model to/from Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_hsHv41JJ2bh",
   "metadata": {
    "id": "_hsHv41JJ2bh"
   },
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vOR57XOeJ2bh",
   "metadata": {
    "id": "vOR57XOeJ2bh"
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2yjGlByVJ2bh",
   "metadata": {
    "id": "2yjGlByVJ2bh"
   },
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L5Csa58uJ2bi",
   "metadata": {
    "id": "L5Csa58uJ2bi"
   },
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7CJhx9-TJ2bh",
   "metadata": {
    "id": "7CJhx9-TJ2bh"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lPwuRJNEJ2bh",
   "metadata": {
    "id": "lPwuRJNEJ2bh"
   },
   "outputs": [],
   "source": [
    "# model_save_name = 'a_great_name'\n",
    "\n",
    "# path = f'/content/gdrive/MyDrive/MPQA/models/{model_save_name}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oOzPfkUAJ2bi",
   "metadata": {
    "id": "oOzPfkUAJ2bi"
   },
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-w-LzOX8UqeF",
   "metadata": {
    "id": "-w-LzOX8UqeF"
   },
   "outputs": [],
   "source": [
    "%mkdir results/preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pYTVRnJv2K5I",
   "metadata": {
    "id": "pYTVRnJv2K5I"
   },
   "outputs": [],
   "source": [
    "# Extract X_val_type from input\n",
    "\n",
    "X_val_type = None\n",
    "if DATASET_NAME in ['MPQA-P', 'MPQA-I']:\n",
    "    X_val_type = [x.split(' | ')[0] for x in X_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QyqPeQnvJ2bi",
   "metadata": {
    "id": "QyqPeQnvJ2bi"
   },
   "outputs": [],
   "source": [
    "if SHOW_VAL_PREDICTIONS == True:\n",
    "\n",
    "    candidates_val = preds_texts_val_list[0][0][-1]\n",
    "    referenceses_val = merge_samples(X_val, targets_texts_val_list[0])\n",
    "    squeezed_X, squeezed_candidates, squeezed_referenceses = squeeze_samples(X_val, candidates_val, referenceses_val)\n",
    "\n",
    "    with open(f'results/preds/{EXPERIMENT_NAME}_val_predictions.txt', 'w', encoding='utf-8') as f:\n",
    "        CNT = 0\n",
    "        for i in range(len(squeezed_X)):\n",
    "            CNT += 1\n",
    "            print(f'\\n#{CNT}', file=f)\n",
    "            print(f'┌X: {repr(squeezed_X[i])}', file=f)\n",
    "            print(f'├C: {repr(squeezed_candidates[i])}', file=f)\n",
    "            print(f'└R: {squeezed_referenceses[i]}', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i2DVUCf0RtT9",
   "metadata": {
    "id": "i2DVUCf0RtT9"
   },
   "outputs": [],
   "source": [
    "# Store raw outputs\n",
    "\n",
    "with open(f'results/preds/{EXPERIMENT_NAME}_raw_val_predictions.txt', 'w', encoding='utf-8') as f:\n",
    "    print('X_val =', X_val, file=f)\n",
    "    print('preds_texts_val_list =', preds_texts_val_list, file=f)\n",
    "    print('targets_texts_val_list =', targets_texts_val_list, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qw5F0H6MYZAS",
   "metadata": {
    "id": "qw5F0H6MYZAS"
   },
   "outputs": [],
   "source": [
    "# Store sampled Data\n",
    "\n",
    "with open(f'results/preds/{EXPERIMENT_NAME}_sampled_data.txt', 'w', encoding='utf-8') as f:\n",
    "    for fold_counter in range(MAX_FOLDS):\n",
    "        print(f'█████ FOLD: {fold_counter}/{MAX_FOLDS-1} ████████████████████████████████████████████████████████████████', file=f)\n",
    "        for seed_counter in range(REPEAT_SAMPLING):\n",
    "            print(f'════╡ {fold_counter} ╞═╡ SEED: {seed_counter}/{REPEAT_SAMPLING-1} ╞═════════════════════════════════════════════════════════', file=f)\n",
    "            for sampling_iteration in range(SAMPLING_ITERATIONS):\n",
    "                print(f'────┤ {fold_counter} ├─┤ {seed_counter} ├─┤ ITER: {sampling_iteration}/{SAMPLING_ITERATIONS-1} ├───────────────────────────────────────────────────', file=f)\n",
    "                Xs, ys = sampled_list[fold_counter][seed_counter][sampling_iteration]\n",
    "                for ax, ay in zip(Xs, ys):\n",
    "                    print(f'┌X: {repr(ax)}', file=f)\n",
    "                    print(f'└y: {repr(ay)}', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NbYC9Q4RUZp_",
   "metadata": {
    "id": "NbYC9Q4RUZp_"
   },
   "outputs": [],
   "source": [
    "if DATASET_NAME == 'MPQA-T':\n",
    "    targets_vecs = outputs_text_to_vec(targets_texts_val_list[0])\n",
    "    preds_vecs   = outputs_text_to_vec(preds_texts_val_list[0][0][-1])\n",
    "\n",
    "    count_targets = np.sum(np.sum(targets_vecs[:, :NUM_TYPE_CLASSES], axis=1) > 1)\n",
    "    count_preds   = np.sum(np.sum(preds_vecs[:, :NUM_TYPE_CLASSES], axis=1) > 1)\n",
    "\n",
    "    print(f' Actual multi-type samples:    {count_targets}\\n Predicted multi-type samples: {count_preds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vBjtB62K02Mj",
   "metadata": {
    "id": "vBjtB62K02Mj"
   },
   "outputs": [],
   "source": [
    "if DATASET_NAME == 'MPQA-I':\n",
    "    preds_vecs = outputs_text_to_vec(preds_texts_val_list[0][0][-1])\n",
    "    for i in range(len(preds_vecs)):\n",
    "        if (preds_vecs[i] == [1, 0, 1]).all():\n",
    "            print(i, preds_vecs[i])\n",
    "        if (preds_vecs[i] == [1, 0, 1]).all():\n",
    "            print(i, preds_vecs[i])\n",
    "        if (preds_vecs[i] == [0, 0, 0]).all():\n",
    "            print(i, preds_vecs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pK5AmBQa4sg8",
   "metadata": {
    "id": "pK5AmBQa4sg8"
   },
   "source": [
    "# Store Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B7_EGzBemWws",
   "metadata": {
    "id": "B7_EGzBemWws"
   },
   "outputs": [],
   "source": [
    "def format_metric_name(name):\n",
    "    \"\"\"\n",
    "    Cleans and abbreviates metric names for CSV headers.\n",
    "    \"\"\"\n",
    "    return name.replace(\"eval_\", \"\") \\\n",
    "               .replace(\"_\", \" \") \\\n",
    "               .replace(\"type\", \"T\") \\\n",
    "               .replace(\"polarity\", \"P\") \\\n",
    "               .replace(\"intensity\", \"I\") \\\n",
    "               .replace(\"accuracy\", \"ACC\") \\\n",
    "               .replace(\"exact match ratio\", \"EMR\") \\\n",
    "               .replace(\"f1\", \"F1\") \\\n",
    "               .replace(\"micro \", \"\") \\\n",
    "               .replace(\"weighted\", \"\") \\\n",
    "               .replace(\"average of metrics\", \"AVG\")\n",
    "\n",
    "csv_file_path = 'results/results.csv'\n",
    "\n",
    "if STORE_RESULTS:\n",
    "    print(f\"STORE_RESULTS detected. Preparing to write results to: {csv_file_path}\")\n",
    "    \n",
    "    # 1. Header Initialization\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        with open(csv_file_path, 'w', encoding='utf-8') as f:\n",
    "            # Basic info headers\n",
    "            f.write('Experiment Name, Iter, ║, ')\n",
    "\n",
    "            for split in ['val', 'tst']:\n",
    "                first_metric = True\n",
    "                for metric_name in STORE_RESULTS:\n",
    "                    if not first_metric:\n",
    "                        f.write('│, ')\n",
    "                    first_metric = False\n",
    "                    \n",
    "                    # Write main metric header\n",
    "                    f.write(f'{split} {format_metric_name(metric_name)}, ')\n",
    "                    \n",
    "                    # Fold-level headers\n",
    "                    if MAX_FOLDS > 1:\n",
    "                        for fold_idx in range(MAX_FOLDS):\n",
    "                            f.write(f'|, fold{fold_idx + 1}, ')\n",
    "                    \n",
    "                    # Seed-level headers\n",
    "                    if REPEAT_SAMPLING > 1:\n",
    "                        f.write('|, ')\n",
    "                        for seed_idx in range(REPEAT_SAMPLING):\n",
    "                            f.write(f'seed{seed_idx}, ')\n",
    "                \n",
    "                f.write('║, ')\n",
    "\n",
    "            f.write('EarlyStopping, Avg Epochs, Best Epoch Avg\\n')\n",
    "\n",
    "    # 2. Data Logging\n",
    "    with open(csv_file_path, 'a', encoding='utf-8') as f:\n",
    "        for iter_idx in range(SAMPLING_ITERATIONS):\n",
    "            # Clean experiment name for CSV safety\n",
    "            clean_exp_name = EXPERIMENT_NAME.replace(\".ipynb\", \"\").replace(\"_\", \" \").replace(\",\", \".\")\n",
    "            f.write(f'{clean_exp_name}, {iter_idx}, ║, ')\n",
    "\n",
    "            for split_log in [folds_val_log, folds_test_log]:\n",
    "                first_metric = True\n",
    "                for metric_name in STORE_RESULTS:\n",
    "                    if not first_metric:\n",
    "                        f.write('│, ')\n",
    "                    first_metric = False\n",
    "                    \n",
    "                    # Safety check for data existence\n",
    "                    if (len(split_log) > 0 and \n",
    "                        len(split_log[0]) > 0 and \n",
    "                        metric_name in split_log[0][0][iter_idx]):\n",
    "                        \n",
    "                        # Extract metrics across folds and seeds\n",
    "                        # Shape: (MAX_FOLDS, REPEAT_SAMPLING)\n",
    "                        metric_values = np.array([\n",
    "                            [split_log[fc][sc][iter_idx][metric_name] for sc in range(REPEAT_SAMPLING)] \n",
    "                            for fc in range(MAX_FOLDS)\n",
    "                        ])\n",
    "                        \n",
    "                        # Write mean across all runs\n",
    "                        f.write(f'{np.round(np.mean(metric_values), 6)}, ')\n",
    "                        \n",
    "                        # Write per-fold and per-seed results\n",
    "                        for fc in range(MAX_FOLDS):\n",
    "                            if MAX_FOLDS > 1:\n",
    "                                f.write(f'|, {np.round(np.mean(metric_values[fc]), 6)}, ')\n",
    "                            \n",
    "                            if REPEAT_SAMPLING > 1:\n",
    "                                f.write('|, ')\n",
    "                                for sc in range(REPEAT_SAMPLING):\n",
    "                                    f.write(f'{np.round(metric_values[fc][sc], 6)}, ')\n",
    "                    else:\n",
    "                        f.write('-, ')\n",
    "                \n",
    "                f.write('║, ')\n",
    "\n",
    "            # Summary statistics (Early Stopping and Epochs)\n",
    "            f.write(f'{np.round(EARLY_STOPPING, 2)}, ')\n",
    "            \n",
    "            # Use itertools to flatten and calculate epoch averages\n",
    "            all_epochs = [\n",
    "                folds_val_log[fc][sc][iter_idx]['epoch'] \n",
    "                for fc, sc in itertools.product(range(MAX_FOLDS), range(REPEAT_SAMPLING))\n",
    "            ]\n",
    "            f.write(f'{np.round(np.mean(all_epochs), 2)}, ')\n",
    "            \n",
    "            best_epochs = [\n",
    "                folds_val_log[fc][sc][iter_idx]['best_epoch'] \n",
    "                for fc, sc in itertools.product(range(MAX_FOLDS), range(REPEAT_SAMPLING))\n",
    "            ]\n",
    "            f.write(f'{np.round(np.mean(best_epochs), 2)}\\n')\n",
    "            \n",
    "    print(\"Results successfully written to CSV.\")\n",
    "else:\n",
    "    print(\"Warning: STORE_RESULTS is empty. CSV writing task skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7jnbajtw4oL8",
   "metadata": {
    "id": "7jnbajtw4oL8"
   },
   "outputs": [],
   "source": [
    "print_log(folds_test_log, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VUPs8IH-4j5V",
   "metadata": {
    "id": "VUPs8IH-4j5V"
   },
   "outputs": [],
   "source": [
    "print_log(folds_val_log, 'Validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oaHBIW8bf2F7",
   "metadata": {
    "id": "oaHBIW8bf2F7"
   },
   "source": [
    "# Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zznzDHU3Z8qO",
   "metadata": {
    "id": "zznzDHU3Z8qO"
   },
   "outputs": [],
   "source": [
    "# Time data\n",
    "\n",
    "end_time = datetime.now() # end timer\n",
    "\n",
    "print('\\033[1mStart:\\033[0m {}'.format(start_time))\n",
    "print('\\033[1mEnd:\\033[0m {}'.format(end_time))\n",
    "print('\\n\\033[1mDuration:\\033[0m {}'.format(end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1ddezX21e1w21PneWCY75i87rWOVgG-gl",
     "timestamp": 1712525609361
    },
    {
     "file_id": "1BI8Chbeio294gsnkYCP8P3mwvsdmZ9BH",
     "timestamp": 1711999606799
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
